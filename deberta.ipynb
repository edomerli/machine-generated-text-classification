{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Human-Written vs Machine-Generated Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run the notebook on Kaggle\n",
    "\n",
    "# !pip install transformers==4.40.1\n",
    "# !pip install evaluate\n",
    "# !pip install bitsandbytes\n",
    "# !pip install peft\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean imports\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed, BitsAndBytesConfig, AutoConfig, AdamW, get_cosine_schedule_with_warmup\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import datetime\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, TaskType, AutoPeftModelForSequenceClassification, prepare_model_for_kbit_training, get_peft_model, PeftModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_model\n",
    "import os.path as path\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases login\n",
    "\n",
    "wandb.login(key=\"14a7d0e7554bbddd13ca1a8d45472f7a95e73ca4\")\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"SemEval8\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/*\n",
    "# TODO: remove this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: riscrivere un po' le variabili / codice per non farlo ricondurre troppo alla baseline lool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data retrieval and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, **fn_kwargs):\n",
    "    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "def get_data(train_path, val_path, test_path):\n",
    "    \"\"\"\n",
    "    function to read dataframe with columns\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_json(train_path, lines=True)\n",
    "    val_df = pd.read_json(val_path, lines=True)\n",
    "    test_df = pd.read_json(test_path, lines=True)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The official evaluation metric for Subtask A is **accuracy**. In the [challenge website](https://www.codabench.org/competitions/1752/#/pages-tab) the scorer also reports **macro-F1** and **micro-F1**.\n",
    "\n",
    "Since it's a binary classification problem though, we limited ourselves to computing **F1-Score**, as it didn't make sense to consider the macro- and micro- interpretations of the metrics (also because the two classes are balanced). (TODO: in realt√† le classi non sono perfettamente bilanciate, quindi magari riguardare le definizioni di micro e macro, che non me le ricordo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"micro\"))\n",
    "    # TODO: remove?\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_and_test(train_df, valid_df, test_df, checkpoints_path, model_name, hparams, predictions_path):\n",
    "    # TODO: documentation\n",
    "\n",
    "    # functions to map labels to ids and vice versa\n",
    "    id2label = {0: \"human\", 1: \"machine\"}\n",
    "    label2id = {\"human\": 0, \"machine\": 1}\n",
    "    \n",
    "    # if train_on_subset is True, then train on a balanced subset of the training data\n",
    "    if hparams['train_on_subset']:\n",
    "        min_samples = train_df[train_df['label'] == 0]['source'].value_counts().min()\n",
    "        train_df = train_df.groupby(['label', 'source']).sample(min_samples, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # pandas dataframe to huggingface Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    valid_dataset = Dataset.from_pandas(valid_df)\n",
    "    \n",
    "    # load tokenizer and model from huggingface using the model name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        device_map = {\"\": 0},\n",
    "        num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    \n",
    "    # if lr_backbone == 0, then freeze the backbone (deberta)\n",
    "    if hparams['lora'] == False and hparams['lr_backbone'] == 0:\n",
    "        for param in model.deberta.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # tokenize data for train/valid\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "\n",
    "    if hparams['lora']:\n",
    "        # parameter-efficient fine-tuning (PEFT) configuration\n",
    "        peft_config = LoraConfig(\n",
    "            lora_alpha=hparams['lora_alpha'],\n",
    "            lora_dropout=0.1,\n",
    "            r=hparams['lora_rank'],\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            target_modules=['query_proj', 'value_proj', 'key_proj', 'dense'],\n",
    "            modules_to_save=[\"score\"]\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, peft_config)\n",
    "        \n",
    "        lora_parameters = []\n",
    "        for param in model.base_model.model.deberta.parameters():\n",
    "            if param.requires_grad:\n",
    "                lora_parameters.append(param)\n",
    "        \n",
    "    output_dir = checkpoints_path\n",
    "    # batches & accumulation\n",
    "    per_device_train_batch_size = hparams['per_device_train_batch_size']\n",
    "    gradient_accumulation_steps = hparams['gradient_accumulation_steps']\n",
    "    per_device_eval_batch_size = per_device_train_batch_size\n",
    "    # number of training steps & validation frequency\n",
    "    max_steps = len(train_df) // per_device_train_batch_size // 2 # because training on 2 GPUs\n",
    "    save_steps = hparams['save_steps']\n",
    "    # optimizer and learning rate\n",
    "    optim=\"adamw_torch\"  # default for TrainingArguments class\n",
    "    warmup_ratio = 0.2\n",
    "    if hparams['lora']:\n",
    "        optimizer=AdamW([{'params': model.base_model.model.classifier.modules_to_save.parameters(), 'lr': hparams['lr_classifier']},\n",
    "                     {'params': lora_parameters}],\n",
    "                    lr=hparams['lr_backbone'])\n",
    "    else:\n",
    "        optimizer=AdamW([{'params': model.classifier.parameters(), 'lr': hparams['lr_classifier']},\n",
    "                     {'params': model.deberta.parameters()}],\n",
    "                    lr=hparams['lr_backbone'])\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(optimizer, max_steps * warmup_ratio, max_steps)\n",
    "    optimizers = (optimizer, lr_scheduler)\n",
    "    learning_rate = hparams['lr_backbone']\n",
    "    lr_scheduler_type = hparams['lr_scheduler_type']\n",
    "    # optimizations\n",
    "    fp16 = True\n",
    "    torch_compile = True\n",
    "    # logging & checkpointing\n",
    "    logging_steps = save_steps\n",
    "    run_name = f\"{model_name}_bs{per_device_train_batch_size}_lr{learning_rate}_{lr_scheduler_type}{'_lora' if hparams['lora'] else ''}{'_subset' if  hparams['train_on_subset'] else '_full'}\"\n",
    "    \n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "\n",
    "    # training phase\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        # number of training steps & validation frequency\n",
    "        max_steps=max_steps,\n",
    "        save_steps=save_steps,\n",
    "        # batches & accumulation\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        # optimizer and learning rate\n",
    "        optim=optim,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        # optimizations\n",
    "        fp16=fp16,\n",
    "        torch_compile=torch_compile,\n",
    "        # logging & checkpointing\n",
    "        logging_steps=logging_steps,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=run_name,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=optimizers\n",
    "    )\n",
    "\n",
    "    # cast normalization layers to float32 for numerical stability\n",
    "    for name, module in trainer.model.named_modules():\n",
    "        if \"norm\" in name:\n",
    "            module = module.to(torch.float32)\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # test phase\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    \n",
    "    predictions_test = trainer.predict(tokenized_test_dataset)\n",
    "    preds_test = np.argmax(predictions_test.predictions, axis=-1)\n",
    "\n",
    "    # TODO: remove below\n",
    "    # bstrai_results = evaluate.load(\"bstrai/classification_report\").compute(predictions=preds_test, references=predictions_test.label_ids)\n",
    "    # TODO: use the same 2 metrics (acc, f1) used when training? save the results as df?\n",
    "    # print(\"Results using classification_report:\", bstrai_results)\n",
    "\n",
    "    results_test = compute_metrics((predictions_test.predictions, predictions_test.label_ids))\n",
    "    print(\"Results using compute_metrics:\", results_test)\n",
    "\n",
    "    # TODO: log test results to wandb\n",
    "    \n",
    "    predictions_df = pd.DataFrame({'id': test_df['id'], 'label': preds_test})\n",
    "    predictions_df.to_csv(predictions_path)\n",
    "    \n",
    "    # 2. save model - TODO: double check if wandb saves the best model, then remove below\n",
    "    # GOAL: avere i checkpoint solo su wandb\n",
    "#     best_model_path = '/kaggle/working/local_save/'\n",
    "    \n",
    "#     if not os.path.exists(best_model_path):\n",
    "#         os.makedirs(best_model_path)\n",
    "    \n",
    "#     # TODO: capire cosa salvare\n",
    "#     trainer.save_model(best_model_path)\n",
    "#     trainer.model.save_pretrained(best_model_path)\n",
    "#     tokenizer.save_pretrained(best_model_path)\n",
    "#     torch.save(trainer.model.score.state_dict(), f'{best_model_path}/score-params.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path =  \"/kaggle/input/subtaskA_train_monolingual.jsonl\" \n",
    "val_path = \"/kaggle/input/subtaskA_dev_monolingual.jsonl\"\n",
    "test_path =  \"/kaggle/input/subtaskA_test_monolingual.jsonl\"\n",
    "\n",
    "train_df, valid_df, test_df = get_data(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [6, 11, 79, 101, 253]   # TODO: every run will use ONE model and a different seed, ONE SEED ONLY, for the big models\n",
    "\n",
    "model_names = ['distilbert-base-uncased', \n",
    "               'distilbert-base-uncased', \n",
    "               'microsoft/deberta-v3-large',\n",
    "               'microsoft/deberta-v3-large']\n",
    "\n",
    "\n",
    "hparams_list = [\n",
    "#   BASELINE 1 ...\n",
    "#   BASELINE 2 ...\n",
    "#   FULLY FINETUNED DEBERTA\n",
    "    {\"per_device_train_batch_size\": 1,\n",
    "     \"gradient_accumulation_steps\": 4,\n",
    "     \"lr_backbone\": 2e-5, \n",
    "     \"lr_classifier\": 2e-3,\n",
    "     \"lr_scheduler_type\": \"cosine\",\n",
    "     \"save_steps\": 3500\n",
    "     \"lora\": False,\n",
    "     \"train_on_subset\": True,\n",
    "    },\n",
    "#   LORA\n",
    "    {\"per_device_train_batch_size\": 4,\n",
    "     \"gradient_accumulation_steps\": 1,\n",
    "     \"lr_backbone\": 8e-5,\n",
    "     \"lr_classifier\": 8e-5,\n",
    "     \"lr_scheduler_type\": \"cosine\",\n",
    "     \"save_steps\": 1000,\n",
    "     \"lora\": True,\n",
    "     \"lora_rank\": 64,\n",
    "     \"lora_alpha\": 16,\n",
    "     \"train_on_subset\": False,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "if train == True:\n",
    "    for model_name, hparams in zip(model_names, hparams_list):\n",
    "        for seed in seeds:\n",
    "            if \"/\" in model_name:\n",
    "                ref_model_name = model_name.split(\"/\")[1]\n",
    "\n",
    "            print(f\"====Training {ref_model_name} with seed {seed}====\")\n",
    "            set_seed(seed)\n",
    "            \n",
    "            ct = datetime.datetime.now()\n",
    "\n",
    "            predictions_path = f'/kaggle/working/subtaskA_predictions_{ref_model_name}_{seed}.csv'\n",
    "\n",
    "            # train and test model\n",
    "            fine_tune_and_test(train_df, valid_df, test_df, f\"models/{ref_model_name}/{seed}/{ct}\", model_name, hparams, predictions_path)\n",
    "else:\n",
    "    print(\"Skipping training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
