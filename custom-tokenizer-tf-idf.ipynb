{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-24T15:35:37.758695Z","iopub.status.busy":"2024-05-24T15:35:37.758322Z","iopub.status.idle":"2024-05-24T15:35:47.097047Z","shell.execute_reply":"2024-05-24T15:35:47.095994Z","shell.execute_reply.started":"2024-05-24T15:35:37.758665Z"},"papermill":{"duration":10.728116,"end_time":"2023-12-18T13:46:44.930628","exception":false,"start_time":"2023-12-18T13:46:34.202512","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Standard Library Imports\n","import sys\n","import gc\n","\n","# Data Handling and Processing\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","\n","# Machine Learning Models\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.svm import LinearSVC, SVC\n","\n","# Model Selection, Model Evaluation and Metrics\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n","\n","# Text Processing and Feature Extraction\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","# Tokenizer and Preprocessing for NLP\n","from transformers import PreTrainedTokenizerFast\n","from tokenizers import (\n","    decoders,\n","    models,\n","    normalizers,\n","    pre_tokenizers,\n","    processors,\n","    trainers,\n","    Tokenizer,\n",")\n","\n","# Dataset Handling and Progress Bar\n","from datasets import Dataset\n","from tqdm.auto import tqdm\n","\n","import string\n","import time\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-24T15:55:32.405191Z","iopub.status.busy":"2024-05-24T15:55:32.404631Z","iopub.status.idle":"2024-05-24T15:55:39.514783Z","shell.execute_reply":"2024-05-24T15:55:39.513445Z","shell.execute_reply.started":"2024-05-24T15:55:32.405157Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of training set: 119757\n","Length of sub-sampled training set: 23570\n"]}],"source":["def get_data(train_path, val_path, test_path):\n","    \"\"\"\n","    function to read dataframe with columns\n","    \"\"\"\n","\n","    train_df = pd.read_json(train_path, lines=True)\n","    val_df = pd.read_json(val_path, lines=True)\n","    test_df = pd.read_json(test_path, lines=True)\n","    \n","    return train_df, val_df, test_df\n","\n","# train_path =  \"/kaggle/input/semeval8-subtask-a/subtaskA_train_monolingual.jsonl\" \n","# val_path = \"/kaggle/input/semeval8-subtask-a/subtaskA_dev_monolingual.jsonl\"\n","# test_path =  \"/kaggle/input/semeval8-subtask-a/subtaskA_test_monolingual.jsonl\"\n","\n","train_path = \"./data/subtaskA_train_monolingual.jsonl\"\n","val_path = \"./data/subtaskA_dev_monolingual.jsonl\"\n","test_path = \"./data/subtaskA_test_monolingual.jsonl\"\n","\n","train, val, test = get_data(train_path, val_path, test_path)\n","\n","min_samples = train[train['label'] == 0]['source'].value_counts().min()\n","train_subset = train.groupby(['label', 'source']).sample(min_samples, random_state=42).reset_index(drop=True)\n","\n","print(f\"Length of training set: {len(train)}\")\n","print(f\"Length of sub-sampled training set: {len(train_subset)}\")\n","\n","print(f\"Proportion of subset training set: {len(train_subset)/len(train)*100}%\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def dummy(text):\n","    \"\"\"\n","    A dummy function to use as tokenizer for TfidfVectorizer. It returns the text as it is since we will have already tokenized it.\n","    \"\"\"\n","    return text"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class TfIdfPipeline:\n","    def __init__(self, train, target, split, lowercase=False):\n","        self.train = train\n","        self.target = target\n","\n","        self.split = split\n","        assert self.split in ['val', 'test'], \"split must be either 'val' or 'test'\"\n","\n","        self.lowercase = lowercase\n","\n","    def trainingset_num_unique_words(self):\n","        \"\"\"\n","        Function to get the number of unique words in the training dataset\n","        \"\"\"\n","        unique_words = set()\n","        for text in self.train['text']:\n","            unique_words.update(text.lower().split())\n","\n","        unique_words = {word.strip(string.punctuation) for word in unique_words}\n","        return len(unique_words)\n","    \n","\n","    def get_tokenizer(self, num_unique_words):\n","        \"\"\"TODO: comment\"\"\"\n","        VOCAB_SIZE = num_unique_words // 2    #TODO: devo provare senza il //2?\n","\n","        # Initializing the tokenizer with Byte-Pair Encoding (BPE) model.\n","        # The [UNK] token is used to represent unknown words during tokenization.\n","        raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n","\n","        # Configuring the tokenizer's normalization and pre-tokenization steps.\n","        # NFC normalization is applied for consistent character representation.\n","        raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if self.lowercase else [])\n","        raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n","\n","        # Specifying special tokens for the tokenizer and initializing the BPE trainer.\n","        # The trainer is configured with the desired vocabulary size and the special tokens.\n","        special_tokens = [\"[UNK]\", \"[SEP]\"]\n","        trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n","\n","        # Converting the val/test data to a Huggingface dataset for easier handling.\n","        target_dataset = Dataset.from_pandas(self.target[['text']])\n","\n","        # Function to generate batches of text data for training.\n","        # This approach helps in managing memory usage when dealing with large datasets.\n","        def target_corpus_iter(): \n","            for i in range(0, len(target_dataset), 1000):\n","                yield target_dataset[i : i + 1000][\"text\"]\n","\n","        # Training the tokenizer on the dataset using the defined trainer.\n","        raw_tokenizer.train_from_iterator(target_corpus_iter(), trainer=trainer)\n","\n","        # Wrapping the trained tokenizer with Huggingface's PreTrainedTokenizerFast for additional functionalities.\n","        # This step integrates the tokenizer with Huggingface's ecosystem, enabling easy use with their models.\n","        tokenizer = PreTrainedTokenizerFast(\n","            tokenizer_object=raw_tokenizer,\n","            unk_token=\"[UNK]\",\n","            sep_token=\"[SEP]\",\n","        )\n","\n","        return tokenizer\n","    \n","    def tokenize(self, tokenizer):\n","        # Tokenizing the text data in the 'train' DataFrame and storing the results.\n","        print(\"Tokenizing training set\")\n","        tokenized_texts_train = []\n","        for text in tqdm(self.train['text'].tolist()):\n","            tokenized_texts_train.append(tokenizer.tokenize(text))\n","\n","        # Tokenizing the text data in the target DataFrame and storing the results.\n","        print(f\"Tokenizing {self.split} set\")\n","        tokenized_texts_target = []\n","        for text in tqdm(self.target['text'].tolist()):\n","            tokenized_texts_target.append(tokenizer.tokenize(text))\n","\n","        return tokenized_texts_train, tokenized_texts_target\n","    \n","    def vectorize(self, tokenized_texts_train, tokenized_texts_target):\n","        # Initialize TfidfVectorizer for val set\n","        # Parameters: \n","        # - ngram_range=(3, 5): Use 3 to 5 word n-grams.\n","        # - lowercase=True/False: Whether to mantain case sensitivity.\n","        # - sublinear_tf=True: Apply sublinear term frequency scaling.\n","        # - analyzer, tokenizer, preprocessor: Use custom 'dummy' functions.\n","        # - token_pattern=None: Disable default token pattern.\n","        # - strip_accents='unicode': Remove accents using Unicode.\n","        vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=self.lowercase, sublinear_tf=True, analyzer='word',\n","                                    tokenizer=dummy, preprocessor=dummy, token_pattern=None, strip_accents='unicode')\n","\n","        # Fit vectorizer on val data to learn vocabulary\n","        print(f\"Fitting Tf-Idf vectorizer to {self.split} set...\", end=\" \")\n","        start = time.time()\n","        vectorizer.fit(tokenized_texts_target)\n","        end = time.time()\n","        print(f\"completed in {round(end - start)} seconds\")\n","        vocab = vectorizer.vocabulary_  # Extract learned vocabulary\n","\n","        # Reinitialize TfidfVectorizer for training set using target set's vocabulary\n","        vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n","                                    analyzer='word', tokenizer=dummy, preprocessor=dummy, token_pattern=None, \n","                                    strip_accents='unicode')\n","\n","        # Transform training and val data into TF-IDF vectors\n","        print(\"Fit-transforming Tf-Idf vectorizer to training set...\", end=\" \")\n","        start = time.time()\n","        tf_train = vectorizer.fit_transform(tokenized_texts_train)\n","        end = time.time()\n","        print(f\"completed in {round(end - start)} seconds\")\n","\n","        print(f\"Transforming {self.split} set...\", end=\" \")\n","        start = time.time()\n","        tf_target = vectorizer.transform(tokenized_texts_target)\n","        end = time.time()\n","        print(f\"completed in {round(end - start)} seconds\")\n","\n","        # Cleanup: Free up memory\n","        del vectorizer\n","        gc.collect()\n","\n","        return tf_train, tf_target\n","    \n","    def run(self):\n","        num_unique_words = self.trainingset_num_unique_words()\n","        tokenizer = self.get_tokenizer(num_unique_words)\n","        tokenized_texts_train, tokenized_texts_target = self.tokenize(tokenizer)\n","        tf_train, tf_target = self.vectorize(tokenized_texts_train, tokenized_texts_target)\n","        return tf_train, tf_target"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seeds = [42, 91, 184, 333, 647]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def hyperparameters_search(tf_train, y_train, tf_val, y_val):\n","    model_names = ['MultinomialNB', 'SVM', 'SGD']\n","    hyperparams = [\n","        {'alpha': [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0]},\n","        {'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0]},\n","        {'alpha': [1e-7, 1e-6, 1e-5, 1e-4, 0.001, 0.01, 0.1]},\n","    ]\n","\n","    best_hyperparams = {model_name: [] for model_name in model_names}\n","    val_accuracies = {model_name: [] for model_name in model_names}\n","    val_f1_scores = {model_name: [] for model_name in model_names}\n","\n","    val_accuracies['Ensemble'] = []\n","    val_f1_scores['Ensemble'] = []\n","\n","    for seed in seeds:\n","        print(f\"======== Training with seed: {seed} =======\")\n","        models = [MultinomialNB(), LinearSVC(max_iter=4000, random_state=seed), SGDClassifier(max_iter=8000, random_state=seed)]\n","\n","        for model_name, model, hyperparam_grid in zip(model_names, models, hyperparams):\n","\n","            gs = GridSearchCV(\n","                estimator=model, \n","                param_grid=hyperparam_grid, \n","                scoring='accuracy',\n","                cv=5,\n","                refit=True,\n","                verbose=True)\n","            \n","            print(f\"Training {model_name}:\", end=\" \")\n","            \n","            start = time.time()\n","            gs.fit(tf_train, y_train)\n","            end = time.time()\n","\n","            print(f\"Completed in {round(end - start)} seconds\")\n","\n","            y_pred = gs.predict(tf_val)\n","            accuracy = accuracy_score(y_val, y_pred)\n","            f1 = f1_score(y_val, y_pred)\n","\n","            best_hyperparams[model_name].append(gs.best_params_)\n","            val_accuracies[model_name].append(accuracy)\n","            val_f1_scores[model_name].append(f1)\n","\n","        print(f\"Training Ensemble: \")\n","\n","        ensemble = VotingClassifier(\n","            estimators=[\n","                ('MultinomialNB', MultinomialNB(**best_hyperparams['MultinomialBN'][-1])),\n","                ('SVM', LinearSVC(**best_hyperparams['SVM'][-1], max_iter=4000, random_state=seed)),\n","                ('SGD', SGDClassifier(**best_hyperparams['SGD'][-1], max_iter=8000, random_state=seed)),\n","            ],\n","            weights=[val_accuracies[model_name][-1] for model_name in model_names], voting='hard', verbose=True)\n","        \n","        ensemble.fit(tf_train, y_train)\n","\n","        gc.collect()\n","\n","        val_preds = ensemble.predict(tf_val)\n","        accuracy = accuracy_score(y_val, val_preds)\n","        f1 = f1_score(y_val, val_preds)\n","\n","        val_accuracies['Ensemble'].append(accuracy)\n","        val_f1_scores['Ensemble'].append(f1)\n","    \n","    return best_hyperparams, val_accuracies, val_f1_scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_random_and_majority_baseline(y_train, y_target, accuracies, f1_scores):\n","    # Random classifier\n","    y_preds = np.random.randint(2, size=len(y_target))\n","    acc = accuracy_score(y_target, y_preds)\n","    f1 = f1_score(y_target, y_preds)\n","    accuracies['Random'] = [acc for _ in seeds]\n","    f1_scores['Random'] = [f1 for _ in seeds]\n","\n","    # Majority classifier\n","    most_common = y_train.value_counts().idxmax()\n","    y_preds = most_common * np.ones_like(y_target)\n","    acc = accuracy_score(y_target, y_preds)\n","    f1 = f1_score(y_target, y_preds)\n","    accuracies['Majority'] = [acc for _ in seeds]\n","    f1_scores['Majority'] = [f1 for _ in seeds]"]},{"cell_type":"markdown","metadata":{},"source":["Using the whole training set"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing training set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1081b47d42254539b7c511933c0a7b9d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/119757 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# find the best hyperparameters for each one of the models using GridSearchCV\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Compute the accuracies on the validation set for each model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Use the accuracies as a vector of scores to build the ensemble model and train it on the training set\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# see the best single models performance on the test set\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# see the ensemble model performance on the test set\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m tf_train, tf_val \u001b[38;5;241m=\u001b[39m \u001b[43mTfIdfPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowercase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     13\u001b[0m best_hyperparams, val_accuracies \u001b[38;5;241m=\u001b[39m hyperparameters_search(tf_train, y_train, tf_val, y_val)\n","Cell \u001b[1;32mIn[7], line 125\u001b[0m, in \u001b[0;36mTfIdfPipeline.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m num_unique_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainingset_num_unique_words()\n\u001b[0;32m    124\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tokenizer(num_unique_words)\n\u001b[1;32m--> 125\u001b[0m tokenized_texts_train, tokenized_texts_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m tf_train, tf_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorize(tokenized_texts_train, tokenized_texts_target)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf_train, tf_target\n","Cell \u001b[1;32mIn[7], line 68\u001b[0m, in \u001b[0;36mTfIdfPipeline.tokenize\u001b[1;34m(self, tokenizer)\u001b[0m\n\u001b[0;32m     66\u001b[0m tokenized_texts_train \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()):\n\u001b[1;32m---> 68\u001b[0m     tokenized_texts_train\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Tokenizing the text data in the target DataFrame and storing the results.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:396\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.tokenize\u001b[1;34m(self, text, pair, add_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, pair: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, add_special_tokens: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtokens()\n","File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2973\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2974\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2979\u001b[0m )\n\u001b[1;32m-> 2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3000\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:576\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    556\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[0;32m    575\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[1;32m--> 576\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n","File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    502\u001b[0m )\n\u001b[1;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[0;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[0;32m    528\u001b[0m ]\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# find the best hyperparameters for each one of the models using GridSearchCV\n","# Compute the accuracies on the validation set for each model\n","# Use the accuracies as a vector of scores to build the ensemble model and train it on the training set\n","# Compute the accuracy of the ensemble model on the validation set (to verify that it is better than the single models)\n","\n","# Initialize the TfIdfPipeline for the test set\n","# see the best single models performance on the test set\n","# see the ensemble model performance on the test set\n","\n","tf_train, tf_val = TfIdfPipeline(train, val, 'val', lowercase=False).run()\n","y_train, y_val = train['label'].values, val['label'].values\n","\n","best_hyperparams, val_accuracies, val_f1_scores = hyperparameters_search(tf_train, y_train, tf_val, y_val)\n","\n","# Add random classifier and majority classifier to the results\n","add_random_and_majority_baseline(y_train, y_val, val_accuracies, val_f1_scores)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_results_table(accuracies, f1_scores):\n","    table = pd.DataFrame({\n","        'Seed': seeds,\n","        'Random Accuracy': accuracies['Random'],\n","        'Random F1': f1_scores['Random'],\n","        'Majority Accuracy': accuracies['Majority'],\n","        'Majority F1': f1_scores['Majority'],\n","        'MultinomialNB Accuracy': accuracies['MultinomialNB'],\n","        'MultinomialNB F1': f1_scores['MultinomialNB'],\n","        'SVM Accuracy': accuracies['SVM'],\n","        'SVM F1': f1_scores['SVM'],\n","        'SGD Accuracy': accuracies['SGD'],\n","        'SGD F1': f1_scores['SGD']\n","        'Ensemble Accuracy': accuracies['Ensemble'],\n","        'Ensemble F1': f1_scores['Ensemble']\n","    })\n","\n","    table['Average Accuracy'] = table[['MultinomialNB Accuracy', 'SVM Accuracy', 'SGD Accuracy', 'Ensemble Accuracy']].mean(axis=1)\n","    table['Average F1'] = table[['MultinomialNB F1', 'SVM F1', 'SGD F1', 'Ensemble F1']].mean(axis=1)\n","\n","    table['Accuracy Std'] = table[['MultinomialNB Accuracy', 'SVM Accuracy', 'SGD Accuracy', 'Ensemble Accuracy']].std(axis=1)\n","    table['F1 Std'] = table[['MultinomialNB F1', 'SVM F1', 'SGD F1', 'Ensemble F1']].std(axis=1)\n","\n","    return table"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_results_table = create_results_table(val_accuracies, val_f1_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_models(best_hyperparameters, tf_train, y_train, tf_test, y_test):\n","    model_names = ['MultinomialNB', 'SVM', 'SGD']\n","\n","    test_accuracies = {model_name: [] for model_name in model_names}\n","    test_f1_scores = {model_name: [] for model_name in model_names}\n","\n","    test_accuracies['Ensemble'] = []\n","    test_f1_scores['Ensemble'] = []\n","\n","    for i, seed in enumerate(seeds):\n","        print(f\"======== Training on test vocabulary with seed: {seed} =======\")\n","\n","        models = [MultinomialNB(), LinearSVC(max_iter=4000, random_state=seed), SGDClassifier(max_iter=8000, random_state=seed)]\n","        for model_name, model in zip(model_names, models):\n","            model.set_params(**best_hyperparameters[model_name][i])\n","\n","            print(f\"Training {model_name}...\", end=\" \")\n","            \n","            start = time.time()\n","            model.fit(tf_train, y_train)\n","            end = time.time()\n","\n","            print(f\"completed in {round(end - start)} seconds\")\n","\n","            y_preds = model.predict(tf_test)\n","            accuracy = accuracy_score(y_test, y_preds)\n","            f1 = f1_score(y_test, y_preds)\n","\n","            test_accuracies[model_name].append(accuracy)\n","            test_f1_scores[model_name].append(f1)\n","\n","        print(f\"Training Ensemble: \")\n","\n","        ensemble = VotingClassifier(\n","            estimators=[\n","                ('MultinomialNB', MultinomialNB(**best_hyperparameters['MultinomialBN'][i])),\n","                ('SVM', LinearSVC(**best_hyperparameters['SVM'][i], max_iter=4000, random_state=seed)),\n","                ('SGD', SGDClassifier(**best_hyperparameters['SGD'][i], max_iter=8000, random_state=seed)),\n","            ],\n","            weights=[test_accuracies[model_name][i] for model_name in model_names], voting='hard', verbose=True)\n","        \n","        ensemble.fit(tf_train, y_train)\n","\n","        gc.collect()\n","\n","        test_preds = ensemble.predict(tf_test)\n","        accuracy = accuracy_score(y_test, test_preds)\n","        f1 = f1_score(y_test, test_preds)\n","\n","        test_accuracies['Ensemble'].append(accuracy)\n","        test_f1_scores['Ensemble'].append(f1)\n","\n","    return test_accuracies, test_f1_scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_train, tf_test = TfIdfPipeline(train, test, 'test', lowercase=False).run()\n","y_train, y_test = train['label'].values, test['label'].values\n","\n","test_accuracies, test_f1_scores = test_models(best_hyperparams, tf_train, y_train, tf_test, y_test)\n","\n","add_random_and_majority_baseline(y_train, y_test, test_accuracies, test_f1_scores)\n","\n","test_results_table = create_results_table(test_accuracies, test_f1_scores)"]},{"cell_type":"markdown","metadata":{},"source":["Using a balanced subset of the training set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_train_subset, tf_val = TfIdfPipeline(train_subset, val, 'val', lowercase=False).run()\n","y_train_subset, y_val = train_subset['label'].values, val['label'].values\n","\n","best_hyperparams, val_accuracies, val_f1_scores = hyperparameters_search(tf_train_subset, y_train_subset, tf_val, y_val)\n","\n","# Add random classifier and majority classifier to the results\n","add_random_and_majority_baseline(y_train_subset, y_val, val_accuracies, val_f1_scores)\n","\n","val_results_table = create_results_table(val_accuracies, val_f1_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_train_subset, tf_test = TfIdfPipeline(train_subset, test, 'test', lowercase=False).run()\n","y_train_subset, y_test = train_subset['label'].values, test['label'].values\n","\n","test_accuracies, test_f1_scores = test_models(best_hyperparams, tf_train_subset, y_train_subset, tf_test, y_test)\n","\n","add_random_and_majority_baseline(y_train_subset, y_test, test_accuracies, test_f1_scores)\n","\n","test_results_table = create_results_table(test_accuracies, test_f1_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# IMPORTANT TODO: bar plots of the accuracies AND F1 SCORES for both validation and test sets -> faccio file pickle delle val e test predictions!\n","# ((invece che delle val e test results tables)). \n","# Poi faccio un file che PLOTTA TUTTO (anche i pickle results tables delle due bert baselines e di deberta LoRA e fullyFineTuned)\n","# Sposto la funzione create_results_table in quel file li!\n","\n","# TODO: confusion matrix for the single and ensemble model on the test set"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4630371,"sourceId":7887441,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"papermill":{"default_parameters":{},"duration":414.966567,"end_time":"2023-12-18T13:53:25.925175","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-18T13:46:30.958608","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0e6fc92c70dc4911bbd7599b098586f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ec47a4894ba40b4a453cf9418155e35","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe78dc5f74d44735b28db3ba1cda7942","value":3}},"18bde72a856549e8a9a80134148000f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ac46d50275e453d8da68aa4265b1509","placeholder":"​","style":"IPY_MODEL_e94cf890b0a943d0859c9e474485e4ea","value":" 3/3 [00:00&lt;00:00, 205.66it/s]"}},"2e87f91f49cb4146b2a4920dafcf13fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea40c6598b36489c89b39dafe041512f","placeholder":"​","style":"IPY_MODEL_677fa584c1554ebb8d8a8ae550244b8c","value":"100%"}},"37c398f54c0d473eaa47ec83602097b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfad0652a82f49a39a88b266ac7bc875","placeholder":"​","style":"IPY_MODEL_59c9726e4898484b84b607a4d0345b5e","value":" 44868/44868 [02:29&lt;00:00, 291.22it/s]"}},"4c0bfc575df64e82904a5d653e04aac1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ec47a4894ba40b4a453cf9418155e35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fdcdf47224c4862a9432fdc71347931":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e87f91f49cb4146b2a4920dafcf13fd","IPY_MODEL_67e4ba7e3c1f44e2a818d85497731f8f","IPY_MODEL_37c398f54c0d473eaa47ec83602097b5"],"layout":"IPY_MODEL_d21def6b586040b89dd9e4ea2e0e60fa"}},"59c9726e4898484b84b607a4d0345b5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ac46d50275e453d8da68aa4265b1509":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c411634a2b840c79ff19e61a1b4e0c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a56b0667f5904801aff412435acc0189","placeholder":"​","style":"IPY_MODEL_4c0bfc575df64e82904a5d653e04aac1","value":"100%"}},"6371df6edf6e4882b18b6d81b621716e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"677fa584c1554ebb8d8a8ae550244b8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67e4ba7e3c1f44e2a818d85497731f8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6371df6edf6e4882b18b6d81b621716e","max":44868,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f84bbd5ec5434cdea7d3283d66cc4b35","value":44868}},"a56b0667f5904801aff412435acc0189":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfd6490b61bd4a3fa9e7ab73466edcd7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c411634a2b840c79ff19e61a1b4e0c2","IPY_MODEL_0e6fc92c70dc4911bbd7599b098586f8","IPY_MODEL_18bde72a856549e8a9a80134148000f8"],"layout":"IPY_MODEL_f00aaa05d1b74d8298036b80cdb11757"}},"cfad0652a82f49a39a88b266ac7bc875":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d21def6b586040b89dd9e4ea2e0e60fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e94cf890b0a943d0859c9e474485e4ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea40c6598b36489c89b39dafe041512f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f00aaa05d1b74d8298036b80cdb11757":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f84bbd5ec5434cdea7d3283d66cc4b35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe78dc5f74d44735b28db3ba1cda7942":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
