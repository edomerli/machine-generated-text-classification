{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Human-Written vs Machine-Generated Text Classification - DistilBERT and DeBERTaV3-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to run the notebook on Kaggle\n",
    "\n",
    "# !pip install transformers==4.40.1\n",
    "# !pip install evaluate\n",
    "# !pip install peft\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed, AdamW, get_cosine_schedule_with_warmup\n",
    "import os\n",
    "import datetime\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases login\n",
    "\n",
    "wandb.login(key=\"14a7d0e7554bbddd13ca1a8d45472f7a95e73ca4\")\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"SemEval8\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data retrieval and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(input, **fn_kwargs):\n",
    "    \"\"\"Function to preprocess input by tokenizing it, truncating to 512 tokens\n",
    "\n",
    "    Args:\n",
    "        input (pandas.DataFrame): the set of texts to tokenize \n",
    "\n",
    "    Returns:\n",
    "        The input texts tokenized\n",
    "    \"\"\"\n",
    "    return fn_kwargs['tokenizer'](input[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "def get_data(train_path, val_path, test_path):\n",
    "    \"\"\"Function to retrieve data from files\n",
    "\n",
    "    Args:\n",
    "        train_path (str): the path to the json training dataset\n",
    "        val_path (str): the path to the json validation dataset\n",
    "        test_path (str): the path to the json test dataset\n",
    "\n",
    "    Returns:\n",
    "        (pandas.Dataframe, pandas.Dataframe, pandas.Dataframe): the respective pandas dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_json(train_path, lines=True)\n",
    "    val_df = pd.read_json(val_path, lines=True)\n",
    "    test_df = pd.read_json(test_path, lines=True)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The official evaluation metric for Subtask A is **accuracy**. In the [challenge website](https://www.codabench.org/competitions/1752/#/pages-tab) the scorer also reports **macro-F1** and **micro-F1**.\n",
    "\n",
    "Since it's a binary classification problem though, we limited ourselves to computing **F1-Score** using \"binary\" as the averaging method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Function to compute accuracy and F1-score metrics on the validation/test predictions\n",
    "\n",
    "    Args:\n",
    "        eval_pred (<np.ndarray, np.ndarray>): the set of predictions and respective labels\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary containing the accuracy and f1-score metrics\n",
    "    \"\"\"\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=predictions, references=labels))\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"binary\"))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"prints the number and percentage of trainable parameters of the model\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): the model of which to print the number of trainable parameters\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup, training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_and_test(train_df, valid_df, test_df, checkpoints_path, model_name, ref_model_name, seed, hparams, predictions_dir):\n",
    "    \"\"\"Main body of the notebook. Trains/fine-tunes the model and saves the validation and test set predictions to a csv file\n",
    "\n",
    "    Args:\n",
    "        train_df (pandas.DataFrame): training dataset\n",
    "        valid_df (pandas.DataFrame): validation dataset\n",
    "        test_df (pandas.DataFrame): test dataset\n",
    "        checkpoints_path (str): the directory where the model checkpoints will be written \n",
    "        model_name (str): the name of the model to train/fine-tune and test\n",
    "        ref_model_name (str): the string representing the model-hyperparameters combination\n",
    "        seed (int): the seed to use for training\n",
    "        hparams (dict): the dictionary containing the set of hyperparameters\n",
    "        predictions_dir (str): path of the directory in which to save the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # functions to map labels to ids and vice versa\n",
    "    id2label = {0: \"human\", 1: \"machine\"}\n",
    "    label2id = {\"human\": 0, \"machine\": 1}\n",
    "    \n",
    "    # if train_on_subset is True, then train on a balanced subset of the training data\n",
    "    if hparams['train_on_subset']:\n",
    "        min_samples = train_df[train_df['label'] == 0]['source'].value_counts().min()\n",
    "        train_df = train_df.groupby(['label', 'source']).sample(min_samples, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # pandas dataframe to huggingface Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    valid_dataset = Dataset.from_pandas(valid_df)\n",
    "    \n",
    "    # load tokenizer and model from huggingface using the model name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        device_map = {\"\": 0},\n",
    "        num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    \n",
    "    # if lr_backbone == 0, then freeze the backbone\n",
    "    if hparams['lora'] == False and hparams['lr_backbone'] == 0:\n",
    "        if 'deberta' in model_name:\n",
    "            for param in model.deberta.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif 'distilbert' in model_name:\n",
    "            for param in model.distilbert.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            raise ValueError(\"Model not supported\")\n",
    "    \n",
    "    # tokenize data for train/valid\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # get the LoRA model if lora hyperparameter is True\n",
    "    if hparams['lora']:\n",
    "        if 'deberta' not in model_name:\n",
    "            raise ValueError(\"LoRA is only supported with DeBERTa models\")\n",
    "        \n",
    "        # parameter-efficient fine-tuning (PEFT) configuration\n",
    "        peft_config = LoraConfig(\n",
    "            lora_alpha=hparams['lora_alpha'],\n",
    "            lora_dropout=0.1,\n",
    "            r=hparams['lora_rank'],\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            target_modules=['query_proj', 'value_proj', 'key_proj', 'dense'],\n",
    "            modules_to_save=[\"score\"]\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, peft_config)\n",
    "        \n",
    "\n",
    "    # TRAINING ARGUMENTS extraction\n",
    "\n",
    "    output_dir = checkpoints_path\n",
    "    # batches & accumulation\n",
    "    per_device_train_batch_size = hparams['per_device_train_batch_size']\n",
    "    gradient_accumulation_steps = hparams['gradient_accumulation_steps']\n",
    "    per_device_eval_batch_size = per_device_train_batch_size\n",
    "    # number of training steps & validation frequency\n",
    "    max_steps = int(hparams['epochs'] * len(train_df)) // per_device_train_batch_size // gradient_accumulation_steps // 2 # because training on 2 GPUs\n",
    "    save_steps = hparams['save_steps']\n",
    "    # optimizer and learning rate\n",
    "    optim=\"adamw_torch\"  # default for TrainingArguments class\n",
    "    warmup_ratio = 0.2\n",
    "    if hparams['lora']:\n",
    "        lora_parameters = []\n",
    "        for param in model.base_model.model.deberta.parameters():\n",
    "            if param.requires_grad:\n",
    "                lora_parameters.append(param)\n",
    "\n",
    "        for param in model.base_model.model.pooler.parameters():\n",
    "            if param.requires_grad:\n",
    "                lora_parameters.append(param)\n",
    "        grouped_parameters = [{'params': model.base_model.model.classifier.modules_to_save.parameters(), 'lr': hparams['lr_classifier']},\n",
    "                              {'params': lora_parameters, 'lr': hparams['lr_backbone']}]\n",
    "    else:\n",
    "        if 'deberta' in model_name:\n",
    "            grouped_parameters = [{'params': model.deberta.parameters(), 'lr': hparams['lr_backbone']},\n",
    "                                  {'params': model.pooler.parameters(), 'lr': hparams['lr_backbone']},\n",
    "                                  {'params': model.classifier.parameters(), 'lr': hparams['lr_classifier']}]\n",
    "        elif 'distilbert' in model_name:\n",
    "            grouped_parameters = [{'params': model.distilbert.parameters(), 'lr': hparams['lr_backbone']}, \n",
    "                                  {'params': model.pre_classifier.parameters(), 'lr': hparams['lr_classifier']},\n",
    "                                  {'params': model.classifier.parameters(), 'lr': hparams['lr_classifier']},]\n",
    "        else:\n",
    "            raise ValueError(\"Model not supported\")\n",
    "    optimizer=AdamW(grouped_parameters)\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(optimizer, max_steps * warmup_ratio, max_steps)\n",
    "    optimizers = (optimizer, lr_scheduler)\n",
    "    learning_rate = hparams['lr_backbone']\n",
    "    lr_scheduler_type = hparams['lr_scheduler_type']\n",
    "    # optimizations\n",
    "    fp16 = True\n",
    "    torch_compile = True\n",
    "    # logging & checkpointing\n",
    "    logging_steps = save_steps\n",
    "    run_name = f\"{ref_model_name}_bs{per_device_train_batch_size}_{'subset' if  hparams['train_on_subset'] else 'full'}_{seed}\"\n",
    "    \n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "\n",
    "    # training phase\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        # number of training steps & validation frequency\n",
    "        max_steps=max_steps,\n",
    "        save_steps=save_steps,\n",
    "        # batches & accumulation\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        # optimizer and learning rate\n",
    "        optim=optim,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        # optimizations\n",
    "        fp16=fp16,\n",
    "        torch_compile=torch_compile,\n",
    "        # logging & checkpointing\n",
    "        logging_steps=logging_steps,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=run_name,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=optimizers\n",
    "    )\n",
    "\n",
    "    # cast normalization layers to float32 for numerical stability\n",
    "    for name, module in trainer.model.named_modules():\n",
    "        if \"norm\" in name:\n",
    "            module = module.to(torch.float32)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    # test phase\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    \n",
    "    # test the model on the test set and compute the metrics\n",
    "    output_test = trainer.predict(tokenized_test_dataset)\n",
    "    results_test = compute_metrics((output_test.predictions, output_test.label_ids))\n",
    "    print(\"Results using compute_metrics:\", results_test)\n",
    "\n",
    "    # log test results to wandb\n",
    "    wandb.run.summary[\"final_test_accuracy\"] = round(results_test[\"accuracy\"], 4)\n",
    "    wandb.run.summary[\"final_test_f1\"] = round(results_test[\"f1\"], 4)\n",
    "\n",
    "    # save the test predictions to a csv file\n",
    "    preds_test = np.argmax(output_test.predictions, axis=-1)\n",
    "    predictions_df = pd.DataFrame({'id': test_df['id'], 'label': preds_test})\n",
    "    predictions_df.to_csv(predictions_dir + f\"subtaskA_test_predictions_{ref_model_name}_{'subset' if  hparams['train_on_subset'] else 'full'}_{seed}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    # repeat for validation set as well, to have that as set of predictions as well\n",
    "    output_val = trainer.predict(tokenized_valid_dataset)\n",
    "    results_val = compute_metrics((output_val.predictions, output_val.label_ids))\n",
    "    print(\"Results using compute_metrics:\", results_val)\n",
    "\n",
    "    wandb.run.summary[\"final_val_accuracy\"] = round(results_val[\"accuracy\"], 4)\n",
    "    wandb.run.summary[\"final_val_f1\"] = round(results_val[\"f1\"], 4)\n",
    "    \n",
    "    preds_val = np.argmax(output_val.predictions, axis=-1)\n",
    "    predictions_df = pd.DataFrame({'id': valid_df['id'], 'label': preds_val})\n",
    "    predictions_df.to_csv(predictions_dir + f\"subtaskA_val_predictions_{ref_model_name}_{'subset' if  hparams['train_on_subset'] else 'full'}_{seed}.csv\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path =  \"/kaggle/input/subtaskA_train_monolingual.jsonl\" \n",
    "val_path = \"/kaggle/input/subtaskA_dev_monolingual.jsonl\"\n",
    "test_path =  \"/kaggle/input/subtaskA_test_monolingual.jsonl\"\n",
    "\n",
    "train_df, valid_df, test_df = get_data(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations and launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\n",
    "    42, \n",
    "    91, \n",
    "    184, \n",
    "    333,\n",
    "    647\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    'distilbert-base-uncased', \n",
    "    'distilbert-base-uncased', \n",
    "    'microsoft/deberta-v3-large',\n",
    "    'microsoft/deberta-v3-large'\n",
    "]\n",
    "\n",
    "reference_names = [\n",
    "    'distilbert_frozen',\n",
    "    'distilbert_finetuned',\n",
    "    'deberta_finetuned',\n",
    "    'deberta_LoRA'\n",
    "]\n",
    "\n",
    "\n",
    "hparams_list = [\n",
    "#   BASELINE 1 - FROZEN BACKBONE DISTILBERT\n",
    "    {\"epochs\": 3,\n",
    "     \"per_device_train_batch_size\": 32,\n",
    "     \"gradient_accumulation_steps\": 1,\n",
    "     \"lr_backbone\": 0, \n",
    "     \"lr_classifier\": 2e-2,\n",
    "     \"lr_scheduler_type\": \"cosine\",\n",
    "     \"save_steps\": 300,\n",
    "     \"lora\": False,\n",
    "     \"train_on_subset\": False,  # True\n",
    "    },\n",
    "#   BASELINE 2 - FULLY FINETUNED DISTILBERT\n",
    "    {\"epochs\": 3,\n",
    "     \"per_device_train_batch_size\": 32,\n",
    "     \"gradient_accumulation_steps\": 1,\n",
    "     \"lr_backbone\": 1e-5,\n",
    "     \"lr_classifier\": 2e-2,\n",
    "     \"lr_scheduler_type\": \"cosine\",\n",
    "     \"save_steps\": 300,\n",
    "     \"lora\": False,\n",
    "     \"train_on_subset\": False,  # True\n",
    "    },\n",
    "#   FULLY FINETUNED DEBERTA\n",
    "    {\"epochs\": 3,\n",
    "     \"per_device_train_batch_size\": 1,\n",
    "     \"gradient_accumulation_steps\": 4,\n",
    "     \"lr_backbone\": 5e-6, \n",
    "     \"lr_classifier\": 1e-2,\n",
    "     \"lr_scheduler_type\": \"cosine\",\n",
    "     \"save_steps\": 4000,\n",
    "     \"lora\": False,\n",
    "     \"train_on_subset\": True,\n",
    "    },\n",
    "#   LORA\n",
    "    {\"epochs\": 1,\n",
    "     \"per_device_train_batch_size\": 4,\n",
    "     \"gradient_accumulation_steps\": 1,\n",
    "     \"lr_backbone\": 8e-5,\n",
    "     \"lr_classifier\": 8e-5,\n",
    "     \"lr_scheduler_type\": \"cosine\",\n",
    "     \"save_steps\": 1000,\n",
    "     \"lora\": True,\n",
    "     \"lora_rank\": 64,\n",
    "     \"lora_alpha\": 16,\n",
    "     \"train_on_subset\": False,  # True\n",
    "    },\n",
    "]\n",
    "\n",
    "predictions_dir = '/kaggle/working/'\n",
    "if train == True:\n",
    "    for model_name, ref_model_name, hparams in zip(model_names, reference_names, hparams_list):\n",
    "        for seed in seeds:\n",
    "            print(f\"==================Training {ref_model_name} with seed {seed}==================\")\n",
    "            set_seed(seed)\n",
    "            \n",
    "            ct = datetime.datetime.now()\n",
    "\n",
    "            # train and test model\n",
    "            fine_tune_and_test(train_df, valid_df, test_df, f\"models/{ref_model_name}/{seed}/{ct}\", model_name, ref_model_name, seed, hparams, predictions_dir)\n",
    "else:\n",
    "    print(\"Skipping training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
