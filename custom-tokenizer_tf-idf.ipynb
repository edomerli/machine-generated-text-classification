{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Binary Human-Written vs Machine-Generated Text Classification - Custom tokenizer + TF-IDF + ML"]},{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-24T15:35:37.758695Z","iopub.status.busy":"2024-05-24T15:35:37.758322Z","iopub.status.idle":"2024-05-24T15:35:47.097047Z","shell.execute_reply":"2024-05-24T15:35:47.095994Z","shell.execute_reply.started":"2024-05-24T15:35:37.758665Z"},"papermill":{"duration":10.728116,"end_time":"2023-12-18T13:46:44.930628","exception":false,"start_time":"2023-12-18T13:46:34.202512","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Standard Library Imports\n","import gc\n","import os\n","\n","# Data Handling and Processing\n","import pandas as pd\n","import numpy as np\n","\n","# Machine Learning Models\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.svm import LinearSVC\n","\n","# Model Selection, Model Evaluation and Metrics\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","# Text Processing and Feature Extraction\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","# Tokenizer and Preprocessing for NLP\n","from transformers import PreTrainedTokenizerFast\n","from tokenizers import (\n","    models,\n","    normalizers,\n","    pre_tokenizers,\n","    trainers,\n","    Tokenizer,\n",")\n","\n","# Dataset Handling and Progress Bar\n","from datasets import Dataset\n","from tqdm.auto import tqdm\n","\n","# Utils\n","import string\n","import time\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data retrieval"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-24T15:55:32.405191Z","iopub.status.busy":"2024-05-24T15:55:32.404631Z","iopub.status.idle":"2024-05-24T15:55:39.514783Z","shell.execute_reply":"2024-05-24T15:55:39.513445Z","shell.execute_reply.started":"2024-05-24T15:55:32.405157Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of training set: 119757\n","Length of sub-sampled training set: 23570\n","Proportion of subset training set: 19.68%\n"]}],"source":["def get_data(train_path, val_path, test_path):\n","    \"\"\"Function to retrieve data from files\n","\n","    Args:\n","        train_path (str): the path to the json training dataset\n","        val_path (str): the path to the json validation dataset\n","        test_path (str): the path to the json test dataset\n","\n","    Returns:\n","        (pandas.Dataframe, pandas.Dataframe, pandas.Dataframe): the respective pandas dataframes\n","    \"\"\"\n","\n","    train_df = pd.read_json(train_path, lines=True)\n","    val_df = pd.read_json(val_path, lines=True)\n","    test_df = pd.read_json(test_path, lines=True)\n","    \n","    return train_df, val_df, test_df\n","\n","train_path = \"./data/subtaskA_train_monolingual.jsonl\"\n","val_path = \"./data/subtaskA_dev_monolingual.jsonl\"\n","test_path = \"./data/subtaskA_test_monolingual.jsonl\"\n","\n","train, val, test = get_data(train_path, val_path, test_path)\n","\n","min_samples = train[train['label'] == 0]['source'].value_counts().min()\n","train_subset = train.groupby(['label', 'source']).sample(min_samples, random_state=42).reset_index(drop=True)\n","\n","print(f\"Length of training set: {len(train)}\")\n","print(f\"Length of sub-sampled training set: {len(train_subset)}\")\n","\n","print(f\"Proportion of subset training set: {round(len(train_subset)/len(train)*100, 2)}%\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def dummy(text):\n","    \"\"\"A dummy function to use as tokenizer for TfidfVectorizer. It returns the text as it is since we will have already tokenized it.\n","    \"\"\"\n","    return text"]},{"cell_type":"markdown","metadata":{},"source":["### Pipeline to run custom tokenizer & compute TF-IDF features"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class TfIdfPipeline:\n","    def __init__(self, train, target, split, lowercase=False):\n","        \"\"\"Constructor for the TfIdfPipeline class\n","\n","        Args:\n","            train (pandas Dataframe): training dataset\n","            target (pandas Dataframe): either validation or test dataset\n","            split (string): either 'val' or 'test', describes the target dataset\n","            lowercase (bool, optional): Whether or not to convert all words to lowercase. Defaults to False.\n","        \"\"\"\n","        self.train = train\n","        self.target = target\n","\n","        self.split = split\n","        assert self.split in ['val', 'test'], \"split must be either 'val' or 'test'\"\n","\n","        self.lowercase = lowercase\n","\n","    def trainingset_num_unique_words(self):\n","        \"\"\"Function to get the number of unique words in the training dataset\n","\n","        Returns:\n","            int: the number of unique words in the training dataset\n","        \"\"\"\n","        unique_words = set()\n","        for text in self.train['text']:\n","            unique_words.update(text.lower().split())\n","\n","        unique_words = {word.strip(string.punctuation) for word in unique_words}\n","        return len(unique_words)\n","    \n","\n","    def get_tokenizer(self, num_unique_words):\n","        \"\"\"Function to get a tokenizer for the dataset\n","\n","        Args:\n","            num_unique_words (int): the number of unique words in the training dataset\n","\n","        Returns:\n","            PreTrainedTokenizerFast: the tokenizer for the dataset\n","        \"\"\"\n","        VOCAB_SIZE = num_unique_words // 2\n","\n","        # Initializing the tokenizer with Byte-Pair Encoding (BPE) model.\n","        # The [UNK] token is used to represent unknown words during tokenization.\n","        raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n","\n","        # Configuring the tokenizer's normalization and pre-tokenization steps.\n","        # NFC normalization is applied for consistent character representation.\n","        raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if self.lowercase else [])\n","        raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n","\n","        # Specifying special tokens for the tokenizer and initializing the BPE trainer.\n","        # The trainer is configured with the desired vocabulary size and the special tokens.\n","        special_tokens = [\"[UNK]\", \"[SEP]\"]\n","        trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n","\n","        # Converting the val/test data to a Huggingface dataset for easier handling.\n","        target_dataset = Dataset.from_pandas(self.target[['text']])\n","\n","        # Function to generate batches of text data for training.\n","        # This approach helps in managing memory usage when dealing with large datasets.\n","        def target_corpus_iter(): \n","            for i in range(0, len(target_dataset), 1000):\n","                yield target_dataset[i : i + 1000][\"text\"]\n","\n","        # Training the tokenizer on the dataset using the defined trainer.\n","        raw_tokenizer.train_from_iterator(target_corpus_iter(), trainer=trainer)\n","\n","        # Wrapping the trained tokenizer with Huggingface's PreTrainedTokenizerFast for additional functionalities.\n","        # This step integrates the tokenizer with Huggingface's ecosystem, enabling easy use with their models.\n","        tokenizer = PreTrainedTokenizerFast(\n","            tokenizer_object=raw_tokenizer,\n","            unk_token=\"[UNK]\",\n","            sep_token=\"[SEP]\",\n","        )\n","\n","        return tokenizer\n","    \n","    def tokenize(self, tokenizer):\n","        \"\"\"Use the tokenizer to tokenize the text data in the training and target datasets\n","\n","        Args:\n","            tokenizer (PreTrainedTokenizerFast): the tokenizer to use\n","\n","        Returns:\n","            (list[list[str]], list[list[str]]): the tokenized text data for the training and target datasets\n","        \"\"\"\n","\n","        # Tokenizing the text data in the 'train' DataFrame and storing the results.\n","        print(\"Tokenizing training set\")\n","        tokenized_texts_train = []\n","        for text in tqdm(self.train['text'].tolist()):\n","            tokenized_texts_train.append(tokenizer.tokenize(text))\n","\n","        # Tokenizing the text data in the target DataFrame and storing the results.\n","        print(f\"Tokenizing {self.split} set\")\n","        tokenized_texts_target = []\n","        for text in tqdm(self.target['text'].tolist()):\n","            tokenized_texts_target.append(tokenizer.tokenize(text))\n","\n","        return tokenized_texts_train, tokenized_texts_target\n","    \n","    def vectorize(self, tokenized_texts_train, tokenized_texts_target):\n","        \"\"\"Function to vectorize the tokenized text data using the TfidfVectorizer\n","\n","        Args:\n","            tokenized_texts_train (list[list[str]]): the tokenized text data for the training dataset\n","            tokenized_texts_target (list[list[str]]): the tokenized text data for the target dataset\n","\n","        Returns:\n","            (scipy.sparse.csr_matrix, scipy.sparse.csr_matrix): the vectorized text data for the training and target datasets\n","        \"\"\"\n","\n","        # Initialize TfidfVectorizer for val set\n","        # Parameters: \n","        # - ngram_range=(3, 5): Use 3 to 5 word n-grams.\n","        # - lowercase=True/False: Whether to mantain case sensitivity.\n","        # - sublinear_tf=True: Apply sublinear term frequency scaling.\n","        # - analyzer, tokenizer, preprocessor: Use custom 'dummy' functions.\n","        # - token_pattern=None: Disable default token pattern.\n","        # - strip_accents='unicode': Remove accents using Unicode.\n","        vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=self.lowercase, sublinear_tf=True, analyzer='word',\n","                                    tokenizer=dummy, preprocessor=dummy, token_pattern=None, strip_accents='unicode')\n","\n","        # Fit vectorizer on val data to learn vocabulary\n","        print(f\"Fitting Tf-Idf vectorizer to {self.split} set...\", end=\" \")\n","        start = time.time()\n","        vectorizer.fit(tokenized_texts_target)\n","        end = time.time()\n","        print(f\"completed in {round(end - start)} seconds\")\n","        vocab = vectorizer.vocabulary_  # Extract learned vocabulary\n","\n","        # Reinitialize TfidfVectorizer for training set using target set's vocabulary\n","        vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n","                                    analyzer='word', tokenizer=dummy, preprocessor=dummy, token_pattern=None, \n","                                    strip_accents='unicode')\n","\n","        # Transform training and val data into TF-IDF vectors\n","        print(\"Fit-transforming Tf-Idf vectorizer to training set...\", end=\" \")\n","        start = time.time()\n","        tf_train = vectorizer.fit_transform(tokenized_texts_train)\n","        end = time.time()\n","        print(f\"completed in {round(end - start)} seconds\")\n","\n","        print(f\"Transforming {self.split} set...\", end=\" \")\n","        start = time.time()\n","        tf_target = vectorizer.transform(tokenized_texts_target)\n","        end = time.time()\n","        print(f\"completed in {round(end - start)} seconds\")\n","\n","        # Cleanup: Free up memory\n","        del vectorizer\n","        gc.collect()\n","\n","        return tf_train, tf_target\n","    \n","    def run(self):\n","        \"\"\"Function to run the pipeline\n","\n","        Returns:\n","            (scipy.sparse.csr_matrix, scipy.sparse.csr_matrix): the vectorized text data for the training and target datasets\n","        \"\"\"\n","        num_unique_words = self.trainingset_num_unique_words()\n","        tokenizer = self.get_tokenizer(num_unique_words)\n","        tokenized_texts_train, tokenized_texts_target = self.tokenize(tokenizer)\n","        tf_train, tf_target = self.vectorize(tokenized_texts_train, tokenized_texts_target)\n","        return tf_train, tf_target"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["seeds = [42 , 91, 184, 333, 647]"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameters search on ML Models"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def hyperparameters_search(tf_train, y_train, tf_val, y_val):\n","    \"\"\"Function to search for the best hyperparameters for the models\n","\n","    Args:\n","        tf_train (scipy.sparse.csr_matrix): the vectorized training text data\n","        y_train (numpy.ndarray): the training labels\n","        tf_val (scipy.sparse.csr_matrix): the vectorized validation text data\n","        y_val (numpy.ndarray): the validation labels\n","\n","    Returns:\n","        (dict, dict, dict): the best hyperparameters & respective validation accuracies and validation f1 scores for the models\n","    \"\"\"\n","    model_names = ['MultinomialNB', 'SVM', 'SGD']\n","    hyperparams = [\n","        {'alpha': [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0]},\n","        {'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0]},\n","        {'alpha': [1e-7, 1e-6, 1e-5, 1e-4, 0.001, 0.01, 0.1]},\n","    ]\n","\n","    best_hyperparams = {model_name: [] for model_name in model_names}\n","    val_accuracies = {model_name: [] for model_name in model_names}\n","    val_f1_scores = {model_name: [] for model_name in model_names}\n","\n","    val_accuracies['Ensemble'] = []\n","    val_f1_scores['Ensemble'] = []\n","\n","    for seed in seeds:\n","        print(f\"======== Training with seed: {seed} =======\")\n","        models = [MultinomialNB(), LinearSVC(max_iter=4000, random_state=seed), SGDClassifier(max_iter=8000, random_state=seed)]\n","\n","        for model_name, model, hyperparam_grid in zip(model_names, models, hyperparams):\n","\n","            gs = GridSearchCV(\n","                estimator=model, \n","                param_grid=hyperparam_grid, \n","                scoring='accuracy',\n","                cv=5,\n","                refit=True,\n","                verbose=True)\n","            \n","            print(f\"Training {model_name}:\", end=\" \")\n","            \n","            start = time.time()\n","            gs.fit(tf_train, y_train)\n","            end = time.time()\n","\n","            print(f\"Completed in {round(end - start)} seconds\")\n","\n","            y_pred = gs.predict(tf_val)\n","            accuracy = accuracy_score(y_val, y_pred)\n","            f1 = f1_score(y_val, y_pred)\n","\n","            best_hyperparams[model_name].append(gs.best_params_)\n","            val_accuracies[model_name].append(accuracy)\n","            val_f1_scores[model_name].append(f1)\n","\n","        print(f\"Training Ensemble: \")\n","\n","        # The weights for the ensemble model are the validation accuracies of the individual models\n","        ensemble = VotingClassifier(\n","            estimators=[\n","                ('MultinomialNB', MultinomialNB(**best_hyperparams['MultinomialNB'][-1])),\n","                ('SVM', LinearSVC(**best_hyperparams['SVM'][-1], max_iter=4000, random_state=seed)),\n","                ('SGD', SGDClassifier(**best_hyperparams['SGD'][-1], max_iter=8000, random_state=seed)),\n","            ],\n","            weights=[val_accuracies[model_name][-1] for model_name in model_names], voting='hard', verbose=True)\n","        \n","        ensemble.fit(tf_train, y_train)\n","\n","        gc.collect()\n","\n","        val_preds = ensemble.predict(tf_val)\n","        accuracy = accuracy_score(y_val, val_preds)\n","        f1 = f1_score(y_val, val_preds)\n","\n","        val_accuracies['Ensemble'].append(accuracy)\n","        val_f1_scores['Ensemble'].append(f1)\n","    \n","    return best_hyperparams, val_accuracies, val_f1_scores"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def add_random_and_majority_baseline(y_train, y_target, accuracies, f1_scores):\n","    \"\"\"Function to add the random and majority classifiers to the accuracies and f1_scores dictionaries\n","\n","    Args:\n","        y_train (numpy.ndarray): the training labels\n","        y_target (numpy.ndarray): the target labels\n","        accuracies (dict): the accuracies of the other models so far\n","        f1_scores (dict): the f1 scores of the other models so far\n","    \"\"\"\n","    # Random classifier\n","    y_preds = np.random.randint(2, size=len(y_target))\n","    acc = accuracy_score(y_target, y_preds)\n","    f1 = f1_score(y_target, y_preds)\n","    accuracies['Random'] = [acc for _ in seeds]\n","    f1_scores['Random'] = [f1 for _ in seeds]\n","\n","    # Majority classifier\n","    most_common = np.bincount(y_train).argmax()\n","    y_preds = most_common * np.ones_like(y_target)\n","    acc = accuracy_score(y_target, y_preds)\n","    f1 = f1_score(y_target, y_preds)\n","    accuracies['Majority'] = [acc for _ in seeds]\n","    f1_scores['Majority'] = [f1 for _ in seeds]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def create_results_table(accuracies, f1_scores):\n","    \"\"\"Function to create the results table\n","\n","    Args:\n","        accuracies (dict): the validation or test accuracies\n","        f1_scores (dict): the validation or test f1 scores\n","\n","    Returns:\n","        pandas.DataFrame: the results table\n","    \"\"\"\n","    table = pd.DataFrame({\n","        'Seed': seeds + ['Mean', 'Std'],\n","        'Random Accuracy': accuracies['Random'] + [np.mean(accuracies['Random']), np.std(accuracies['Random'])],\n","        'Random F1': f1_scores['Random'] + [np.mean(f1_scores['Random']), np.std(f1_scores['Random'])],\n","        'Majority Accuracy': accuracies['Majority'] + [np.mean(accuracies['Majority']), np.std(accuracies['Majority'])],\n","        'Majority F1': f1_scores['Majority'] + [np.mean(f1_scores['Majority']), np.std(f1_scores['Majority'])],\n","        'MultinomialNB Accuracy': accuracies['MultinomialNB'] + [np.mean(accuracies['MultinomialNB']), np.std(accuracies['MultinomialNB'])],\n","        'MultinomialNB F1': f1_scores['MultinomialNB'] + [np.mean(f1_scores['MultinomialNB']), np.std(f1_scores['MultinomialNB'])],\n","        'SVM Accuracy': accuracies['SVM'] + [np.mean(accuracies['SVM']), np.std(accuracies['SVM'])],\n","        'SVM F1': f1_scores['SVM'] + [np.mean(f1_scores['SVM']), np.std(f1_scores['SVM'])],\n","        'SGD Accuracy': accuracies['SGD'] + [np.mean(accuracies['SGD']), np.std(accuracies['SGD'])],\n","        'SGD F1': f1_scores['SGD'] + [np.mean(f1_scores['SGD']), np.std(f1_scores['SGD'])],\n","        'Ensemble Accuracy': accuracies['Ensemble'] + [np.mean(accuracies['Ensemble']), np.std(accuracies['Ensemble'])],\n","        'Ensemble F1': f1_scores['Ensemble'] + [np.mean(f1_scores['Ensemble']), np.std(f1_scores['Ensemble'])],\n","    }) \n","\n","    return table"]},{"cell_type":"markdown","metadata":{},"source":["Run using the whole training set"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing training set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"149a9f11c3aa4dfab693ae46ac85c393","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/119757 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenizing val set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba756d05b89f45329d59b23556965e6b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fitting Tf-Idf vectorizer to val set... completed in 64 seconds\n","Fit-transforming Tf-Idf vectorizer to training set... Unexpected exception formatting exception. Falling back to standard exception\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"C:\\Users\\merli\\AppData\\Local\\Temp\\ipykernel_11720\\225727777.py\", line 1, in <module>\n","    tf_train, tf_val = TfIdfPipeline(train, val, 'val', lowercase=False).run()\n","                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\merli\\AppData\\Local\\Temp\\ipykernel_11720\\3121380320.py\", line 167, in run\n","    tf_train, tf_target = self.vectorize(tokenized_texts_train, tokenized_texts_target)\n","                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"C:\\Users\\merli\\AppData\\Local\\Temp\\ipykernel_11720\\3121380320.py\", line 142, in vectorize\n","    tf_train = vectorizer.fit_transform(tokenized_texts_train)\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2091, in fit_transform\n","    X = super().fit_transform(raw_documents)\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1372, in fit_transform\n","    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line -1, in _count_vocab\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2142, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n","    frames.append(self.format_record(record))\n","                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n","    frame_info.lines, Colors, self.has_colors, lvals\n","    ^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n","    return self._sd.lines\n","           ^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n","    pieces = self.included_pieces\n","             ^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n","    pos = scope_pieces.index(self.executing_piece)\n","                             ^^^^^^^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n","    return only(\n","           ^^^^^\n","  File \"d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\executing\\executing.py\", line 116, in only\n","    raise NotOneValueFound('Expected one value, found 0')\n","executing.executing.NotOneValueFound: Expected one value, found 0\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["tf_train, tf_val = TfIdfPipeline(train, val, 'val', lowercase=False).run()\n","y_train, y_val = train['label'].values, val['label'].values\n","\n","best_hyperparams, val_accuracies, val_f1_scores = hyperparameters_search(tf_train, y_train, tf_val, y_val)\n","\n","# Add random classifier and majority classifier to the results\n","add_random_and_majority_baseline(y_train, y_val, val_accuracies, val_f1_scores)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_results_table = create_results_table(val_accuracies, val_f1_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_results_table"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Save the results to a csv file\n","results_dir = './predictions'\n","\n","if not os.path.exists(results_dir):\n","    os.makedirs(results_dir)\n","\n","val_results_table.to_csv(f'{results_dir}/tfidf_val_results.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_models(best_hyperparameters, tf_train, y_train, tf_test, y_test, val_accuracies):\n","    \"\"\"Function to test the models on the test dataset\n","\n","    Args:\n","        best_hyperparameters (dict): the best hyperparameters for the models\n","        tf_train (scipy.sparse.csr_matrix): the vectorized training text data\n","        y_train (numpy.ndarray): the training labels\n","        tf_test (scipy.sparse.csr_matrix): the vectorized test text data\n","        y_test (numpy.ndarray): the test labels\n","        val_accuracies (dict): the validation accuracies of the models, used to weight the ensemble model\n","\n","    Returns:\n","        (dict, dict): the test accuracies and test f1 scores for the models\n","    \"\"\"\n","    model_names = ['MultinomialNB', 'SVM', 'SGD']\n","\n","    test_accuracies = {model_name: [] for model_name in model_names}\n","    test_f1_scores = {model_name: [] for model_name in model_names}\n","\n","    test_accuracies['Ensemble'] = []\n","    test_f1_scores['Ensemble'] = []\n","\n","    for i, seed in enumerate(seeds):\n","        print(f\"======== Training on test vocabulary with seed: {seed} =======\")\n","\n","        models = [MultinomialNB(), LinearSVC(max_iter=4000, random_state=seed), SGDClassifier(max_iter=8000, random_state=seed)]\n","        for model_name, model in zip(model_names, models):\n","            # Set the best hyperparameters for the model\n","            model.set_params(**best_hyperparameters[model_name][i])\n","\n","            # Train the model on the training data\n","            print(f\"Training {model_name}...\", end=\" \")\n","            \n","            start = time.time()\n","            model.fit(tf_train, y_train)\n","            end = time.time()\n","\n","            print(f\"completed in {round(end - start)} seconds\")\n","\n","            # Test the model on the test data\n","            y_preds = model.predict(tf_test)\n","            accuracy = accuracy_score(y_test, y_preds)\n","            f1 = f1_score(y_test, y_preds)\n","\n","            test_accuracies[model_name].append(accuracy)\n","            test_f1_scores[model_name].append(f1)\n","\n","        # Train the ensemble model on the training data, using the validation accuracies as weights\n","        print(f\"Training Ensemble: \")\n","\n","        ensemble = VotingClassifier(\n","            estimators=[\n","                ('MultinomialNB', MultinomialNB(**best_hyperparameters['MultinomialNB'][i])),\n","                ('SVM', LinearSVC(**best_hyperparameters['SVM'][i], max_iter=4000, random_state=seed)),\n","                ('SGD', SGDClassifier(**best_hyperparameters['SGD'][i], max_iter=8000, random_state=seed)),\n","            ],\n","            weights=[val_accuracies[model_name][i] for model_name in model_names], voting='hard', verbose=True)\n","        \n","        ensemble.fit(tf_train, y_train)\n","\n","        gc.collect()\n","\n","        # Test the ensemble model on the test data\n","        test_preds = ensemble.predict(tf_test)\n","        accuracy = accuracy_score(y_test, test_preds)\n","        f1 = f1_score(y_test, test_preds)\n","\n","        test_accuracies['Ensemble'].append(accuracy)\n","        test_f1_scores['Ensemble'].append(f1)\n","\n","    return test_accuracies, test_f1_scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_train, tf_test = TfIdfPipeline(train, test, 'test', lowercase=False).run()\n","y_train, y_test = train['label'].values, test['label'].values\n","\n","test_accuracies, test_f1_scores = test_models(best_hyperparams, tf_train, y_train, tf_test, y_test, val_accuracies)\n","\n","add_random_and_majority_baseline(y_train, y_test, test_accuracies, test_f1_scores)\n","\n","test_results_table = create_results_table(test_accuracies, test_f1_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save the results in a csv file\n","test_results_table.to_csv(f'{results_dir}/tfidf_test_results_full.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_results_table"]},{"cell_type":"markdown","metadata":{},"source":["Run using a balanced subset of the training set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_train_subset, tf_val = TfIdfPipeline(train_subset, val, 'val', lowercase=False).run()\n","y_train_subset, y_val = train_subset['label'].values, val['label'].values\n","\n","best_hyperparams, val_accuracies, val_f1_scores = hyperparameters_search(tf_train_subset, y_train_subset, tf_val, y_val)\n","\n","# Add random classifier and majority classifier to the results\n","add_random_and_majority_baseline(y_train_subset, y_val, val_accuracies, val_f1_scores)\n","\n","val_results_table = create_results_table(val_accuracies, val_f1_scores)\n","\n","val_results_table.to_csv(f'{results_dir}/tfidf_val_results_subset.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_train_subset, tf_test = TfIdfPipeline(train_subset, test, 'test', lowercase=False).run()\n","y_train_subset, y_test = train_subset['label'].values, test['label'].values\n","\n","test_accuracies, test_f1_scores = test_models(best_hyperparams, tf_train_subset, y_train_subset, tf_test, y_test, val_accuracies)\n","\n","add_random_and_majority_baseline(y_train_subset, y_test, test_accuracies, test_f1_scores)\n","\n","test_results_table = create_results_table(test_accuracies, test_f1_scores)\n","\n","test_results_table.to_csv(f'{results_dir}/tfidf_test_results_subset.csv')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4630371,"sourceId":7887441,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"papermill":{"default_parameters":{},"duration":414.966567,"end_time":"2023-12-18T13:53:25.925175","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-18T13:46:30.958608","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0e6fc92c70dc4911bbd7599b098586f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ec47a4894ba40b4a453cf9418155e35","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe78dc5f74d44735b28db3ba1cda7942","value":3}},"18bde72a856549e8a9a80134148000f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ac46d50275e453d8da68aa4265b1509","placeholder":"​","style":"IPY_MODEL_e94cf890b0a943d0859c9e474485e4ea","value":" 3/3 [00:00&lt;00:00, 205.66it/s]"}},"2e87f91f49cb4146b2a4920dafcf13fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea40c6598b36489c89b39dafe041512f","placeholder":"​","style":"IPY_MODEL_677fa584c1554ebb8d8a8ae550244b8c","value":"100%"}},"37c398f54c0d473eaa47ec83602097b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfad0652a82f49a39a88b266ac7bc875","placeholder":"​","style":"IPY_MODEL_59c9726e4898484b84b607a4d0345b5e","value":" 44868/44868 [02:29&lt;00:00, 291.22it/s]"}},"4c0bfc575df64e82904a5d653e04aac1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ec47a4894ba40b4a453cf9418155e35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fdcdf47224c4862a9432fdc71347931":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e87f91f49cb4146b2a4920dafcf13fd","IPY_MODEL_67e4ba7e3c1f44e2a818d85497731f8f","IPY_MODEL_37c398f54c0d473eaa47ec83602097b5"],"layout":"IPY_MODEL_d21def6b586040b89dd9e4ea2e0e60fa"}},"59c9726e4898484b84b607a4d0345b5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ac46d50275e453d8da68aa4265b1509":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c411634a2b840c79ff19e61a1b4e0c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a56b0667f5904801aff412435acc0189","placeholder":"​","style":"IPY_MODEL_4c0bfc575df64e82904a5d653e04aac1","value":"100%"}},"6371df6edf6e4882b18b6d81b621716e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"677fa584c1554ebb8d8a8ae550244b8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67e4ba7e3c1f44e2a818d85497731f8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6371df6edf6e4882b18b6d81b621716e","max":44868,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f84bbd5ec5434cdea7d3283d66cc4b35","value":44868}},"a56b0667f5904801aff412435acc0189":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfd6490b61bd4a3fa9e7ab73466edcd7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c411634a2b840c79ff19e61a1b4e0c2","IPY_MODEL_0e6fc92c70dc4911bbd7599b098586f8","IPY_MODEL_18bde72a856549e8a9a80134148000f8"],"layout":"IPY_MODEL_f00aaa05d1b74d8298036b80cdb11757"}},"cfad0652a82f49a39a88b266ac7bc875":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d21def6b586040b89dd9e4ea2e0e60fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e94cf890b0a943d0859c9e474485e4ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea40c6598b36489c89b39dafe041512f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f00aaa05d1b74d8298036b80cdb11757":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f84bbd5ec5434cdea7d3283d66cc4b35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe78dc5f74d44735b28db3ba1cda7942":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
