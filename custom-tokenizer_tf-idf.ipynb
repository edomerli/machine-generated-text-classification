{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Binary Human-Written vs Machine-Generated Text Classification - Custom tokenizer + TF-IDF + ML"]},{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-24T15:35:37.758695Z","iopub.status.busy":"2024-05-24T15:35:37.758322Z","iopub.status.idle":"2024-05-24T15:35:47.097047Z","shell.execute_reply":"2024-05-24T15:35:47.095994Z","shell.execute_reply.started":"2024-05-24T15:35:37.758665Z"},"papermill":{"duration":10.728116,"end_time":"2023-12-18T13:46:44.930628","exception":false,"start_time":"2023-12-18T13:46:34.202512","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Standard Library Imports\n","import gc\n","import os\n","\n","# Data Handling and Processing\n","import pandas as pd\n","import numpy as np\n","\n","# Machine Learning Models\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.svm import LinearSVC\n","\n","# Model Selection, Model Evaluation and Metrics\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","# Text Processing and Feature Extraction\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","# Tokenizer and Preprocessing for NLP\n","from transformers import PreTrainedTokenizerFast\n","from tokenizers import (\n","    models,\n","    normalizers,\n","    pre_tokenizers,\n","    trainers,\n","    Tokenizer,\n",")\n","\n","# Dataset Handling and Progress Bar\n","from datasets import Dataset\n","from tqdm.auto import tqdm\n","\n","# Utils\n","import string\n","import time\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data retrieval"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-24T15:55:32.405191Z","iopub.status.busy":"2024-05-24T15:55:32.404631Z","iopub.status.idle":"2024-05-24T15:55:39.514783Z","shell.execute_reply":"2024-05-24T15:55:39.513445Z","shell.execute_reply.started":"2024-05-24T15:55:32.405157Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of training set: 119757\n","Length of sub-sampled training set: 23570\n","Proportion of subset training set: 19.68%\n"]}],"source":["def get_data(train_path, val_path, test_path):\n","    \"\"\"Function to retrieve data from files\n","\n","    Args:\n","        train_path (str): the path to the json training dataset\n","        val_path (str): the path to the json validation dataset\n","        test_path (str): the path to the json test dataset\n","\n","    Returns:\n","        (pandas.Dataframe, pandas.Dataframe, pandas.Dataframe): the respective pandas dataframes\n","    \"\"\"\n","\n","    train_df = pd.read_json(train_path, lines=True)\n","    val_df = pd.read_json(val_path, lines=True)\n","    test_df = pd.read_json(test_path, lines=True)\n","    \n","    return train_df, val_df, test_df\n","\n","train_path = \"./data/subtaskA_train_monolingual.jsonl\"\n","val_path = \"./data/subtaskA_dev_monolingual.jsonl\"\n","test_path = \"./data/subtaskA_test_monolingual.jsonl\"\n","\n","train, val, test = get_data(train_path, val_path, test_path)\n","\n","min_samples = train[train['label'] == 0]['source'].value_counts().min()\n","train_subset = train.groupby(['label', 'source']).sample(min_samples, random_state=42).reset_index(drop=True)\n","\n","print(f\"Length of training set: {len(train)}\")\n","print(f\"Length of sub-sampled training set: {len(train_subset)}\")\n","\n","print(f\"Proportion of subset training set: {round(len(train_subset)/len(train)*100, 2)}%\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def dummy(text):\n","    \"\"\"A dummy function to use as tokenizer for TfidfVectorizer. It returns the text as it is since we will have already tokenized it.\n","    \"\"\"\n","    return text"]},{"cell_type":"markdown","metadata":{},"source":["### Pipeline to run custom tokenizer & compute TF-IDF features"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class TfIdfPipeline:\n","    def __init__(self, train, target, split, lowercase=False):\n","        \"\"\"Constructor for the TfIdfPipeline class\n","\n","        Args:\n","            train (pandas Dataframe): training dataset\n","            target (pandas Dataframe): either validation or test dataset\n","            split (string): either 'val' or 'test', describes the target dataset\n","            lowercase (bool, optional): Whether or not to convert all words to lowercase. Defaults to False.\n","        \"\"\"\n","        self.train = train\n","        self.target = target\n","\n","        self.split = split\n","        assert self.split in ['val', 'test'], \"split must be either 'val' or 'test'\"\n","\n","        self.lowercase = lowercase\n","\n","    def trainingset_num_unique_words(self):\n","        \"\"\"Function to get the number of unique words in the training dataset\n","\n","        Returns:\n","            int: the number of unique words in the training dataset\n","        \"\"\"\n","        unique_words = set()\n","        for text in self.train['text']:\n","            unique_words.update(text.lower().split())\n","\n","        unique_words = {word.strip(string.punctuation) for word in unique_words}\n","        return len(unique_words)\n","    \n","\n","    def get_tokenizer(self, num_unique_words):\n","        \"\"\"Function to get a tokenizer for the dataset\n","\n","        Args:\n","            num_unique_words (int): the number of unique words in the training dataset\n","\n","        Returns:\n","            PreTrainedTokenizerFast: the tokenizer for the dataset\n","        \"\"\"\n","        VOCAB_SIZE = num_unique_words // 2\n","\n","        # Initializing the tokenizer with Byte-Pair Encoding (BPE) model.\n","        # The [UNK] token is used to represent unknown words during tokenization.\n","        raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n","\n","        # Configuring the tokenizer's normalization and pre-tokenization steps.\n","        # NFC normalization is applied for consistent character representation.\n","        raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if self.lowercase else [])\n","        raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n","\n","        # Specifying special tokens for the tokenizer and initializing the BPE trainer.\n","        # The trainer is configured with the desired vocabulary size and the special tokens.\n","        special_tokens = [\"[UNK]\", \"[SEP]\"]\n","        trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n","\n","        # Converting the val/test data to a Huggingface dataset for easier handling.\n","        target_dataset = Dataset.from_pandas(self.target[['text']])\n","\n","        # Function to generate batches of text data for training.\n","        # This approach helps in managing memory usage when dealing with large datasets.\n","        def target_corpus_iter(): \n","            for i in range(0, len(target_dataset), 1000):\n","                yield target_dataset[i : i + 1000][\"text\"]\n","\n","        # Training the tokenizer on the dataset using the defined trainer.\n","        raw_tokenizer.train_from_iterator(target_corpus_iter(), trainer=trainer)\n","\n","        # Wrapping the trained tokenizer with Huggingface's PreTrainedTokenizerFast for additional functionalities.\n","        # This step integrates the tokenizer with Huggingface's ecosystem, enabling easy use with their models.\n","        tokenizer = PreTrainedTokenizerFast(\n","            tokenizer_object=raw_tokenizer,\n","            unk_token=\"[UNK]\",\n","            sep_token=\"[SEP]\",\n","        )\n","\n","        return tokenizer\n","    \n","    def tokenize(self, tokenizer):\n","        \"\"\"Use the tokenizer to tokenize the text data in the training and target datasets\n","\n","        Args:\n","            tokenizer (PreTrainedTokenizerFast): the tokenizer to use\n","\n","        Returns:\n","            (list[list[str]], list[list[str]]): the tokenized text data for the training and target datasets\n","        \"\"\"\n","\n","        # Tokenizing the text data in the 'train' DataFrame and storing the results.\n","        print(\"Tokenizing training set\")\n","        tokenized_texts_train = []\n","        for text in tqdm(self.train['text'].tolist()):\n","            tokenized_texts_train.append(tokenizer.tokenize(text))\n","\n","        # Tokenizing the text data in the target DataFrame and storing the results.\n","        print(f\"Tokenizing {self.split} set\")\n","        tokenized_texts_target = []\n","        for text in tqdm(self.target['text'].tolist()):\n","            tokenized_texts_target.append(tokenizer.tokenize(text))\n","\n","        return tokenized_texts_train, tokenized_texts_target\n","    \n","    def vectorize(self, tokenized_texts_train, tokenized_texts_target):\n","        \"\"\"Function to vectorize the tokenized text data using the TfidfVectorizer\n","\n","        Args:\n","            tokenized_texts_train (list[list[str]]): the tokenized text data for the training dataset\n","            tokenized_texts_target (list[list[str]]): the tokenized text data for the target dataset\n","\n","        Returns:\n","            (scipy.sparse.csr_matrix, scipy.sparse.csr_matrix): the vectorized text data for the training and target datasets\n","        \"\"\"\n","\n","        # Initialize TfidfVectorizer for val set\n","        # Parameters: \n","        # - ngram_range=(3, 5): Use 3 to 5 word n-grams.\n","        # - lowercase=True/False: Whether to mantain case sensitivity.\n","        # - sublinear_tf=True: Apply sublinear term frequency scaling.\n","        # - analyzer, tokenizer, preprocessor: Use custom 'dummy' functions.\n","        # - token_pattern=None: Disable default token pattern.\n","        # - strip_accents='unicode': Remove accents using Unicode.\n","        vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=self.lowercase, sublinear_tf=True, analyzer='word',\n","                                    tokenizer=dummy, preprocessor=dummy, token_pattern=None, strip_accents='unicode')\n","\n","        # Fit vectorizer on val data to learn vocabulary\n","        print(f\"Fitting Tf-Idf vectorizer to {self.split} set...\", end=\" \")\n","        start = time.time()\n","        vectorizer.fit(tokenized_texts_target)\n","        end = time.time()\n","        print(f\"completed in {round(end - start)} seconds\")\n","        vocab = vectorizer.vocabulary_  # Extract learned vocabulary\n","\n","        # Reinitialize TfidfVectorizer for training set using target set's vocabulary\n","        vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n","                                    analyzer='word', tokenizer=dummy, preprocessor=dummy, token_pattern=None, \n","                                    strip_accents='unicode')\n","\n","        # Transform training and val data into TF-IDF vectors\n","        print(\"Fit-transforming Tf-Idf vectorizer to training set...\", end=\" \")\n","        start = time.time()\n","        tf_train = vectorizer.fit_transform(tokenized_texts_train)\n","        end = time.time()\n","        print(f\"completed in {round(end - start)} seconds\")\n","\n","        print(f\"Transforming {self.split} set...\", end=\" \")\n","        start = time.time()\n","        tf_target = vectorizer.transform(tokenized_texts_target)\n","        end = time.time()\n","        print(f\"completed in {round(end - start)} seconds\")\n","\n","        # Cleanup: Free up memory\n","        del vectorizer\n","        gc.collect()\n","\n","        return tf_train, tf_target\n","    \n","    def run(self):\n","        \"\"\"Function to run the pipeline\n","\n","        Returns:\n","            (scipy.sparse.csr_matrix, scipy.sparse.csr_matrix): the vectorized text data for the training and target datasets\n","        \"\"\"\n","        num_unique_words = self.trainingset_num_unique_words()\n","        tokenizer = self.get_tokenizer(num_unique_words)\n","        tokenized_texts_train, tokenized_texts_target = self.tokenize(tokenizer)\n","        tf_train, tf_target = self.vectorize(tokenized_texts_train, tokenized_texts_target)\n","        return tf_train, tf_target"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["seeds = [42 , 91, 184, 333, 647]"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameters search on ML Models"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def hyperparameters_search(tf_train, y_train, tf_val, y_val):\n","    \"\"\"Function to search for the best hyperparameters for the models\n","\n","    Args:\n","        tf_train (scipy.sparse.csr_matrix): the vectorized training text data\n","        y_train (numpy.ndarray): the training labels\n","        tf_val (scipy.sparse.csr_matrix): the vectorized validation text data\n","        y_val (numpy.ndarray): the validation labels\n","\n","    Returns:\n","        (dict, dict, dict): the best hyperparameters & respective validation accuracies and validation f1 scores for the models\n","    \"\"\"\n","    model_names = ['MultinomialNB', 'SVM', 'SGD']\n","    hyperparams = [\n","        {'alpha': [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0]},\n","        {'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0]},\n","        {'alpha': [1e-7, 1e-6, 1e-5, 1e-4, 0.001, 0.01, 0.1]},\n","    ]\n","\n","    best_hyperparams = {model_name: [] for model_name in model_names}\n","    val_accuracies = {model_name: [] for model_name in model_names}\n","    val_f1_scores = {model_name: [] for model_name in model_names}\n","\n","    val_accuracies['Ensemble'] = []\n","    val_f1_scores['Ensemble'] = []\n","\n","    for seed in seeds:\n","        print(f\"======== Training with seed: {seed} =======\")\n","        models = [MultinomialNB(), LinearSVC(max_iter=4000, random_state=seed), SGDClassifier(max_iter=8000, random_state=seed)]\n","\n","        for model_name, model, hyperparam_grid in zip(model_names, models, hyperparams):\n","\n","            gs = GridSearchCV(\n","                estimator=model, \n","                param_grid=hyperparam_grid, \n","                scoring='accuracy',\n","                cv=5,\n","                refit=True,\n","                verbose=True)\n","            \n","            print(f\"Training {model_name}:\", end=\" \")\n","            \n","            start = time.time()\n","            gs.fit(tf_train, y_train)\n","            end = time.time()\n","\n","            print(f\"Completed in {round(end - start)} seconds\")\n","\n","            y_pred = gs.predict(tf_val)\n","            accuracy = accuracy_score(y_val, y_pred)\n","            f1 = f1_score(y_val, y_pred)\n","\n","            best_hyperparams[model_name].append(gs.best_params_)\n","            val_accuracies[model_name].append(accuracy)\n","            val_f1_scores[model_name].append(f1)\n","\n","        print(f\"Training Ensemble: \")\n","\n","        # The weights for the ensemble model are the validation accuracies of the individual models\n","        ensemble = VotingClassifier(\n","            estimators=[\n","                ('MultinomialNB', MultinomialNB(**best_hyperparams['MultinomialNB'][-1])),\n","                ('SVM', LinearSVC(**best_hyperparams['SVM'][-1], max_iter=4000, random_state=seed)),\n","                ('SGD', SGDClassifier(**best_hyperparams['SGD'][-1], max_iter=8000, random_state=seed)),\n","            ],\n","            weights=[val_accuracies[model_name][-1] for model_name in model_names], voting='hard', verbose=True)\n","        \n","        ensemble.fit(tf_train, y_train)\n","\n","        gc.collect()\n","\n","        val_preds = ensemble.predict(tf_val)\n","        accuracy = accuracy_score(y_val, val_preds)\n","        f1 = f1_score(y_val, val_preds)\n","\n","        val_accuracies['Ensemble'].append(accuracy)\n","        val_f1_scores['Ensemble'].append(f1)\n","    \n","    return best_hyperparams, val_accuracies, val_f1_scores"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def add_random_and_majority_baseline(y_train, y_target, accuracies, f1_scores):\n","    \"\"\"Function to add the random and majority classifiers to the accuracies and f1_scores dictionaries\n","\n","    Args:\n","        y_train (numpy.ndarray): the training labels\n","        y_target (numpy.ndarray): the target labels\n","        accuracies (dict): the accuracies of the other models so far\n","        f1_scores (dict): the f1 scores of the other models so far\n","    \"\"\"\n","    # Random classifier\n","    y_preds = np.random.randint(2, size=len(y_target))\n","    acc = accuracy_score(y_target, y_preds)\n","    f1 = f1_score(y_target, y_preds)\n","    accuracies['Random'] = [acc for _ in seeds]\n","    f1_scores['Random'] = [f1 for _ in seeds]\n","\n","    # Majority classifier\n","    most_common = np.bincount(y_train).argmax()\n","    y_preds = most_common * np.ones_like(y_target)\n","    acc = accuracy_score(y_target, y_preds)\n","    f1 = f1_score(y_target, y_preds)\n","    accuracies['Majority'] = [acc for _ in seeds]\n","    f1_scores['Majority'] = [f1 for _ in seeds]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def create_results_table(accuracies, f1_scores):\n","    \"\"\"Function to create the results table\n","\n","    Args:\n","        accuracies (dict): the validation or test accuracies\n","        f1_scores (dict): the validation or test f1 scores\n","\n","    Returns:\n","        pandas.DataFrame: the results table\n","    \"\"\"\n","    table = pd.DataFrame({\n","        'Seed': seeds + ['Mean', 'Std'],\n","        'Random Accuracy': accuracies['Random'] + [np.mean(accuracies['Random']), np.std(accuracies['Random'])],\n","        'Random F1': f1_scores['Random'] + [np.mean(f1_scores['Random']), np.std(f1_scores['Random'])],\n","        'Majority Accuracy': accuracies['Majority'] + [np.mean(accuracies['Majority']), np.std(accuracies['Majority'])],\n","        'Majority F1': f1_scores['Majority'] + [np.mean(f1_scores['Majority']), np.std(f1_scores['Majority'])],\n","        'MultinomialNB Accuracy': accuracies['MultinomialNB'] + [np.mean(accuracies['MultinomialNB']), np.std(accuracies['MultinomialNB'])],\n","        'MultinomialNB F1': f1_scores['MultinomialNB'] + [np.mean(f1_scores['MultinomialNB']), np.std(f1_scores['MultinomialNB'])],\n","        'SVM Accuracy': accuracies['SVM'] + [np.mean(accuracies['SVM']), np.std(accuracies['SVM'])],\n","        'SVM F1': f1_scores['SVM'] + [np.mean(f1_scores['SVM']), np.std(f1_scores['SVM'])],\n","        'SGD Accuracy': accuracies['SGD'] + [np.mean(accuracies['SGD']), np.std(accuracies['SGD'])],\n","        'SGD F1': f1_scores['SGD'] + [np.mean(f1_scores['SGD']), np.std(f1_scores['SGD'])],\n","        'Ensemble Accuracy': accuracies['Ensemble'] + [np.mean(accuracies['Ensemble']), np.std(accuracies['Ensemble'])],\n","        'Ensemble F1': f1_scores['Ensemble'] + [np.mean(f1_scores['Ensemble']), np.std(f1_scores['Ensemble'])],\n","    }) \n","\n","    return table"]},{"cell_type":"markdown","metadata":{},"source":["Run using the whole training set"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing training set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40b8ebcc4b8841e5951af6673b064151","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/119757 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenizing val set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf59503d86564d1cb3c39c4178f1fc96","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fitting Tf-Idf vectorizer to val set... completed in 27 seconds\n","Fit-transforming Tf-Idf vectorizer to training set... completed in 165 seconds\n","Transforming val set... completed in 7 seconds\n","======== Training with seed: 42 =======\n","Training MultinomialNB: Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Completed in 39 seconds\n","Training SVM: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 311 seconds\n","Training SGD: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 63 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.6s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  16.4s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   2.4s\n","======== Training with seed: 91 =======\n","Training MultinomialNB: Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Completed in 40 seconds\n","Training SVM: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 305 seconds\n","Training SGD: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 64 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.6s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  16.1s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   2.7s\n","======== Training with seed: 184 =======\n","Training MultinomialNB: Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Completed in 40 seconds\n","Training SVM: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 306 seconds\n","Training SGD: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 64 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.6s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  16.2s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   2.6s\n","======== Training with seed: 333 =======\n","Training MultinomialNB: Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Completed in 40 seconds\n","Training SVM: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 307 seconds\n","Training SGD: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 65 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.6s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  16.2s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   2.6s\n","======== Training with seed: 647 =======\n","Training MultinomialNB: Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Completed in 40 seconds\n","Training SVM: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 304 seconds\n","Training SGD: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 63 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.6s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  15.9s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   2.6s\n"]}],"source":["tf_train, tf_val = TfIdfPipeline(train, val, 'val', lowercase=False).run()\n","y_train, y_val = train['label'].values, val['label'].values\n","\n","best_hyperparams, val_accuracies, val_f1_scores = hyperparameters_search(tf_train, y_train, tf_val, y_val)\n","\n","# Add random classifier and majority classifier to the results\n","add_random_and_majority_baseline(y_train, y_val, val_accuracies, val_f1_scores)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["val_results_table = create_results_table(val_accuracies, val_f1_scores)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Seed</th>\n","      <th>Random Accuracy</th>\n","      <th>Random F1</th>\n","      <th>Majority Accuracy</th>\n","      <th>Majority F1</th>\n","      <th>MultinomialNB Accuracy</th>\n","      <th>MultinomialNB F1</th>\n","      <th>SVM Accuracy</th>\n","      <th>SVM F1</th>\n","      <th>SGD Accuracy</th>\n","      <th>SGD F1</th>\n","      <th>Ensemble Accuracy</th>\n","      <th>Ensemble F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>42</td>\n","      <td>0.4968</td>\n","      <td>4.961954e-01</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.6554</td>\n","      <td>0.7207</td>\n","      <td>8.634000e-01</td>\n","      <td>0.874702</td>\n","      <td>0.840600</td>\n","      <td>0.858713</td>\n","      <td>0.843600</td>\n","      <td>0.860507</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>91</td>\n","      <td>0.4968</td>\n","      <td>4.961954e-01</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.6554</td>\n","      <td>0.7207</td>\n","      <td>8.634000e-01</td>\n","      <td>0.874702</td>\n","      <td>0.833600</td>\n","      <td>0.853881</td>\n","      <td>0.837200</td>\n","      <td>0.855980</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>184</td>\n","      <td>0.4968</td>\n","      <td>4.961954e-01</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.6554</td>\n","      <td>0.7207</td>\n","      <td>8.634000e-01</td>\n","      <td>0.874702</td>\n","      <td>0.841800</td>\n","      <td>0.859926</td>\n","      <td>0.844200</td>\n","      <td>0.861215</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333</td>\n","      <td>0.4968</td>\n","      <td>4.961954e-01</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.6554</td>\n","      <td>0.7207</td>\n","      <td>8.634000e-01</td>\n","      <td>0.874702</td>\n","      <td>0.841600</td>\n","      <td>0.859525</td>\n","      <td>0.844000</td>\n","      <td>0.860814</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>647</td>\n","      <td>0.4968</td>\n","      <td>4.961954e-01</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.6554</td>\n","      <td>0.7207</td>\n","      <td>8.634000e-01</td>\n","      <td>0.874702</td>\n","      <td>0.836200</td>\n","      <td>0.855581</td>\n","      <td>0.839400</td>\n","      <td>0.857447</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Mean</td>\n","      <td>0.4968</td>\n","      <td>4.961954e-01</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.6554</td>\n","      <td>0.7207</td>\n","      <td>8.634000e-01</td>\n","      <td>0.874702</td>\n","      <td>0.838760</td>\n","      <td>0.857525</td>\n","      <td>0.841680</td>\n","      <td>0.859193</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Std</td>\n","      <td>0.0000</td>\n","      <td>5.551115e-17</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>1.110223e-16</td>\n","      <td>0.000000</td>\n","      <td>0.003282</td>\n","      <td>0.002376</td>\n","      <td>0.002853</td>\n","      <td>0.002089</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Seed  Random Accuracy     Random F1  Majority Accuracy  Majority F1  \\\n","0    42           0.4968  4.961954e-01                0.5          0.0   \n","1    91           0.4968  4.961954e-01                0.5          0.0   \n","2   184           0.4968  4.961954e-01                0.5          0.0   \n","3   333           0.4968  4.961954e-01                0.5          0.0   \n","4   647           0.4968  4.961954e-01                0.5          0.0   \n","5  Mean           0.4968  4.961954e-01                0.5          0.0   \n","6   Std           0.0000  5.551115e-17                0.0          0.0   \n","\n","   MultinomialNB Accuracy  MultinomialNB F1  SVM Accuracy    SVM F1  \\\n","0                  0.6554            0.7207  8.634000e-01  0.874702   \n","1                  0.6554            0.7207  8.634000e-01  0.874702   \n","2                  0.6554            0.7207  8.634000e-01  0.874702   \n","3                  0.6554            0.7207  8.634000e-01  0.874702   \n","4                  0.6554            0.7207  8.634000e-01  0.874702   \n","5                  0.6554            0.7207  8.634000e-01  0.874702   \n","6                  0.0000            0.0000  1.110223e-16  0.000000   \n","\n","   SGD Accuracy    SGD F1  Ensemble Accuracy  Ensemble F1  \n","0      0.840600  0.858713           0.843600     0.860507  \n","1      0.833600  0.853881           0.837200     0.855980  \n","2      0.841800  0.859926           0.844200     0.861215  \n","3      0.841600  0.859525           0.844000     0.860814  \n","4      0.836200  0.855581           0.839400     0.857447  \n","5      0.838760  0.857525           0.841680     0.859193  \n","6      0.003282  0.002376           0.002853     0.002089  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["val_results_table"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Save the results to a csv file\n","results_dir = './predictions'\n","\n","if not os.path.exists(results_dir):\n","    os.makedirs(results_dir)\n","\n","val_results_table.to_csv(f'{results_dir}/tfidf_val_results.csv')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def test_models(best_hyperparameters, tf_train, y_train, tf_test, y_test, val_accuracies):\n","    \"\"\"Function to test the models on the test dataset\n","\n","    Args:\n","        best_hyperparameters (dict): the best hyperparameters for the models\n","        tf_train (scipy.sparse.csr_matrix): the vectorized training text data\n","        y_train (numpy.ndarray): the training labels\n","        tf_test (scipy.sparse.csr_matrix): the vectorized test text data\n","        y_test (numpy.ndarray): the test labels\n","        val_accuracies (dict): the validation accuracies of the models, used to weight the ensemble model\n","\n","    Returns:\n","        (dict, dict): the test accuracies and test f1 scores for the models\n","    \"\"\"\n","    model_names = ['MultinomialNB', 'SVM', 'SGD']\n","\n","    test_accuracies = {model_name: [] for model_name in model_names}\n","    test_f1_scores = {model_name: [] for model_name in model_names}\n","\n","    test_accuracies['Ensemble'] = []\n","    test_f1_scores['Ensemble'] = []\n","\n","    for i, seed in enumerate(seeds):\n","        print(f\"======== Training on test vocabulary with seed: {seed} =======\")\n","\n","        models = [MultinomialNB(), LinearSVC(max_iter=4000, random_state=seed), SGDClassifier(max_iter=8000, random_state=seed)]\n","        for model_name, model in zip(model_names, models):\n","            # Set the best hyperparameters for the model\n","            model.set_params(**best_hyperparameters[model_name][i])\n","\n","            # Train the model on the training data\n","            print(f\"Training {model_name}...\", end=\" \")\n","            \n","            start = time.time()\n","            model.fit(tf_train, y_train)\n","            end = time.time()\n","\n","            print(f\"completed in {round(end - start)} seconds\")\n","\n","            # Test the model on the test data\n","            y_preds = model.predict(tf_test)\n","            accuracy = accuracy_score(y_test, y_preds)\n","            f1 = f1_score(y_test, y_preds)\n","\n","            test_accuracies[model_name].append(accuracy)\n","            test_f1_scores[model_name].append(f1)\n","\n","        # Train the ensemble model on the training data, using the validation accuracies as weights\n","        print(f\"Training Ensemble: \")\n","\n","        ensemble = VotingClassifier(\n","            estimators=[\n","                ('MultinomialNB', MultinomialNB(**best_hyperparameters['MultinomialNB'][i])),\n","                ('SVM', LinearSVC(**best_hyperparameters['SVM'][i], max_iter=4000, random_state=seed)),\n","                ('SGD', SGDClassifier(**best_hyperparameters['SGD'][i], max_iter=8000, random_state=seed)),\n","            ],\n","            weights=[val_accuracies[model_name][i] for model_name in model_names], voting='hard', verbose=True)\n","        \n","        ensemble.fit(tf_train, y_train)\n","\n","        gc.collect()\n","\n","        # Test the ensemble model on the test data\n","        test_preds = ensemble.predict(tf_test)\n","        accuracy = accuracy_score(y_test, test_preds)\n","        f1 = f1_score(y_test, test_preds)\n","\n","        test_accuracies['Ensemble'].append(accuracy)\n","        test_f1_scores['Ensemble'].append(f1)\n","\n","    return test_accuracies, test_f1_scores"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing training set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94682d365e3742639e058301fdc7fc45","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/119757 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenizing test set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f47c57cd7bc74993b1622e974213b463","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/34272 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fitting Tf-Idf vectorizer to test set... completed in 163 seconds\n","Fit-transforming Tf-Idf vectorizer to training set... completed in 227 seconds\n","Transforming test set... completed in 66 seconds\n","======== Training on test vocabulary with seed: 42 =======\n","Training MultinomialNB... completed in 1 seconds\n","Training SVM... completed in 28 seconds\n","Training SGD... completed in 4 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   1.3s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  29.2s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   4.3s\n","======== Training on test vocabulary with seed: 91 =======\n","Training MultinomialNB... completed in 1 seconds\n","Training SVM... completed in 28 seconds\n","Training SGD... completed in 4 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   1.4s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  28.0s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   4.4s\n","======== Training on test vocabulary with seed: 184 =======\n","Training MultinomialNB... completed in 1 seconds\n","Training SVM... completed in 29 seconds\n","Training SGD... completed in 4 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   1.4s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  28.6s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   4.3s\n","======== Training on test vocabulary with seed: 333 =======\n","Training MultinomialNB... completed in 1 seconds\n","Training SVM... completed in 28 seconds\n","Training SGD... completed in 4 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   1.3s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  28.3s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   4.3s\n","======== Training on test vocabulary with seed: 647 =======\n","Training MultinomialNB... completed in 1 seconds\n","Training SVM... completed in 28 seconds\n","Training SGD... completed in 4 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   1.4s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  28.3s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   4.3s\n"]}],"source":["tf_train, tf_test = TfIdfPipeline(train, test, 'test', lowercase=False).run()\n","y_train, y_test = train['label'].values, test['label'].values\n","\n","test_accuracies, test_f1_scores = test_models(best_hyperparams, tf_train, y_train, tf_test, y_test, val_accuracies)\n","\n","add_random_and_majority_baseline(y_train, y_test, test_accuracies, test_f1_scores)\n","\n","test_results_table = create_results_table(test_accuracies, test_f1_scores)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# save the results in a csv file\n","test_results_table.to_csv(f'{results_dir}/tfidf_test_results_full.csv')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Seed</th>\n","      <th>Random Accuracy</th>\n","      <th>Random F1</th>\n","      <th>Majority Accuracy</th>\n","      <th>Majority F1</th>\n","      <th>MultinomialNB Accuracy</th>\n","      <th>MultinomialNB F1</th>\n","      <th>SVM Accuracy</th>\n","      <th>SVM F1</th>\n","      <th>SGD Accuracy</th>\n","      <th>SGD F1</th>\n","      <th>Ensemble Accuracy</th>\n","      <th>Ensemble F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>42</td>\n","      <td>0.501809</td>\n","      <td>0.51486</td>\n","      <td>0.47479</td>\n","      <td>0.0</td>\n","      <td>0.86222</td>\n","      <td>8.726812e-01</td>\n","      <td>0.720968</td>\n","      <td>0.778249</td>\n","      <td>0.664478</td>\n","      <td>0.750926</td>\n","      <td>0.719363</td>\n","      <td>0.779424</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>91</td>\n","      <td>0.501809</td>\n","      <td>0.51486</td>\n","      <td>0.47479</td>\n","      <td>0.0</td>\n","      <td>0.86222</td>\n","      <td>8.726812e-01</td>\n","      <td>0.720968</td>\n","      <td>0.778249</td>\n","      <td>0.651494</td>\n","      <td>0.744699</td>\n","      <td>0.718750</td>\n","      <td>0.779292</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>184</td>\n","      <td>0.501809</td>\n","      <td>0.51486</td>\n","      <td>0.47479</td>\n","      <td>0.0</td>\n","      <td>0.86222</td>\n","      <td>8.726812e-01</td>\n","      <td>0.720968</td>\n","      <td>0.778249</td>\n","      <td>0.655725</td>\n","      <td>0.746580</td>\n","      <td>0.718983</td>\n","      <td>0.779303</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333</td>\n","      <td>0.501809</td>\n","      <td>0.51486</td>\n","      <td>0.47479</td>\n","      <td>0.0</td>\n","      <td>0.86222</td>\n","      <td>8.726812e-01</td>\n","      <td>0.720968</td>\n","      <td>0.778249</td>\n","      <td>0.652136</td>\n","      <td>0.744766</td>\n","      <td>0.718546</td>\n","      <td>0.779076</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>647</td>\n","      <td>0.501809</td>\n","      <td>0.51486</td>\n","      <td>0.47479</td>\n","      <td>0.0</td>\n","      <td>0.86222</td>\n","      <td>8.726812e-01</td>\n","      <td>0.720968</td>\n","      <td>0.778249</td>\n","      <td>0.652194</td>\n","      <td>0.744939</td>\n","      <td>0.718633</td>\n","      <td>0.779180</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Mean</td>\n","      <td>0.501809</td>\n","      <td>0.51486</td>\n","      <td>0.47479</td>\n","      <td>0.0</td>\n","      <td>0.86222</td>\n","      <td>8.726812e-01</td>\n","      <td>0.720968</td>\n","      <td>0.778249</td>\n","      <td>0.655205</td>\n","      <td>0.746382</td>\n","      <td>0.718855</td>\n","      <td>0.779255</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Std</td>\n","      <td>0.000000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.0</td>\n","      <td>0.00000</td>\n","      <td>1.110223e-16</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.004869</td>\n","      <td>0.002375</td>\n","      <td>0.000293</td>\n","      <td>0.000118</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Seed  Random Accuracy  Random F1  Majority Accuracy  Majority F1  \\\n","0    42         0.501809    0.51486            0.47479          0.0   \n","1    91         0.501809    0.51486            0.47479          0.0   \n","2   184         0.501809    0.51486            0.47479          0.0   \n","3   333         0.501809    0.51486            0.47479          0.0   \n","4   647         0.501809    0.51486            0.47479          0.0   \n","5  Mean         0.501809    0.51486            0.47479          0.0   \n","6   Std         0.000000    0.00000            0.00000          0.0   \n","\n","   MultinomialNB Accuracy  MultinomialNB F1  SVM Accuracy    SVM F1  \\\n","0                 0.86222      8.726812e-01      0.720968  0.778249   \n","1                 0.86222      8.726812e-01      0.720968  0.778249   \n","2                 0.86222      8.726812e-01      0.720968  0.778249   \n","3                 0.86222      8.726812e-01      0.720968  0.778249   \n","4                 0.86222      8.726812e-01      0.720968  0.778249   \n","5                 0.86222      8.726812e-01      0.720968  0.778249   \n","6                 0.00000      1.110223e-16      0.000000  0.000000   \n","\n","   SGD Accuracy    SGD F1  Ensemble Accuracy  Ensemble F1  \n","0      0.664478  0.750926           0.719363     0.779424  \n","1      0.651494  0.744699           0.718750     0.779292  \n","2      0.655725  0.746580           0.718983     0.779303  \n","3      0.652136  0.744766           0.718546     0.779076  \n","4      0.652194  0.744939           0.718633     0.779180  \n","5      0.655205  0.746382           0.718855     0.779255  \n","6      0.004869  0.002375           0.000293     0.000118  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["test_results_table"]},{"cell_type":"markdown","metadata":{},"source":["Run using a balanced subset of the training set"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing training set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e30f643e55b4412a3b1ef7df782d132","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/23570 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenizing val set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a2b85b7c1294334a8fdf94251110749","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fitting Tf-Idf vectorizer to val set... completed in 24 seconds\n","Fit-transforming Tf-Idf vectorizer to training set... completed in 32 seconds\n","Transforming val set... completed in 7 seconds\n","======== Training with seed: 42 =======\n","Training MultinomialNB: Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Completed in 20 seconds\n","Training SVM: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 94 seconds\n","Training SGD: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 14 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.3s\n","[Voting] ...................... (2 of 3) Processing SVM, total=   9.4s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   0.4s\n","======== Training with seed: 91 =======\n","Training MultinomialNB: Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Completed in 20 seconds\n","Training SVM: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 93 seconds\n","Training SGD: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 14 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.3s\n","[Voting] ...................... (2 of 3) Processing SVM, total=   9.5s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   0.5s\n","======== Training with seed: 184 =======\n","Training MultinomialNB: Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Completed in 20 seconds\n","Training SVM: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 93 seconds\n","Training SGD: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 14 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.3s\n","[Voting] ...................... (2 of 3) Processing SVM, total=   9.3s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   0.4s\n","======== Training with seed: 333 =======\n","Training MultinomialNB: Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Completed in 20 seconds\n","Training SVM: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 96 seconds\n","Training SGD: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 14 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.3s\n","[Voting] ...................... (2 of 3) Processing SVM, total=   9.7s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   0.4s\n","======== Training with seed: 647 =======\n","Training MultinomialNB: Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","Completed in 19 seconds\n","Training SVM: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 95 seconds\n","Training SGD: Fitting 5 folds for each of 7 candidates, totalling 35 fits\n","Completed in 14 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.3s\n","[Voting] ...................... (2 of 3) Processing SVM, total=   9.8s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   0.4s\n"]}],"source":["tf_train_subset, tf_val = TfIdfPipeline(train_subset, val, 'val', lowercase=False).run()\n","y_train_subset, y_val = train_subset['label'].values, val['label'].values\n","\n","best_hyperparams, val_accuracies, val_f1_scores = hyperparameters_search(tf_train_subset, y_train_subset, tf_val, y_val)\n","\n","# Add random classifier and majority classifier to the results\n","add_random_and_majority_baseline(y_train_subset, y_val, val_accuracies, val_f1_scores)\n","\n","val_results_table = create_results_table(val_accuracies, val_f1_scores)\n","\n","val_results_table.to_csv(f'{results_dir}/tfidf_val_results_subset.csv')"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing training set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8e357e897fa4870b8d6f112da046fa8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/23570 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenizing test set\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"324597231c8248c7bad6b6d9cfe9035d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/34272 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Fitting Tf-Idf vectorizer to test set... completed in 153 seconds\n","Fit-transforming Tf-Idf vectorizer to training set... completed in 40 seconds\n","Transforming test set... completed in 54 seconds\n","======== Training on test vocabulary with seed: 42 =======\n","Training MultinomialNB... completed in 1 seconds\n","Training SVM... completed in 16 seconds\n","Training SGD... completed in 1 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   1.0s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  16.3s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   0.8s\n","======== Training on test vocabulary with seed: 91 =======\n","Training MultinomialNB... completed in 1 seconds\n","Training SVM... completed in 16 seconds\n","Training SGD... completed in 1 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   1.0s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  16.3s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   0.8s\n","======== Training on test vocabulary with seed: 184 =======\n","Training MultinomialNB... completed in 1 seconds\n","Training SVM... completed in 16 seconds\n","Training SGD... completed in 1 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   1.0s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  16.1s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   0.8s\n","======== Training on test vocabulary with seed: 333 =======\n","Training MultinomialNB... completed in 1 seconds\n","Training SVM... completed in 16 seconds\n","Training SGD... completed in 1 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   1.0s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  16.6s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   0.8s\n","======== Training on test vocabulary with seed: 647 =======\n","Training MultinomialNB... completed in 1 seconds\n","Training SVM... completed in 16 seconds\n","Training SGD... completed in 1 seconds\n","Training Ensemble: \n","[Voting] ............ (1 of 3) Processing MultinomialNB, total=   0.9s\n","[Voting] ...................... (2 of 3) Processing SVM, total=  16.1s\n","[Voting] ...................... (3 of 3) Processing SGD, total=   0.7s\n"]}],"source":["tf_train_subset, tf_test = TfIdfPipeline(train_subset, test, 'test', lowercase=False).run()\n","y_train_subset, y_test = train_subset['label'].values, test['label'].values\n","\n","test_accuracies, test_f1_scores = test_models(best_hyperparams, tf_train_subset, y_train_subset, tf_test, y_test, val_accuracies)\n","\n","add_random_and_majority_baseline(y_train_subset, y_test, test_accuracies, test_f1_scores)\n","\n","test_results_table = create_results_table(test_accuracies, test_f1_scores)\n","\n","test_results_table.to_csv(f'{results_dir}/tfidf_test_results_subset.csv')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4630371,"sourceId":7887441,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"papermill":{"default_parameters":{},"duration":414.966567,"end_time":"2023-12-18T13:53:25.925175","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-18T13:46:30.958608","version":"2.4.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0e6fc92c70dc4911bbd7599b098586f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ec47a4894ba40b4a453cf9418155e35","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe78dc5f74d44735b28db3ba1cda7942","value":3}},"18bde72a856549e8a9a80134148000f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ac46d50275e453d8da68aa4265b1509","placeholder":"","style":"IPY_MODEL_e94cf890b0a943d0859c9e474485e4ea","value":" 3/3 [00:00&lt;00:00, 205.66it/s]"}},"2e87f91f49cb4146b2a4920dafcf13fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea40c6598b36489c89b39dafe041512f","placeholder":"","style":"IPY_MODEL_677fa584c1554ebb8d8a8ae550244b8c","value":"100%"}},"37c398f54c0d473eaa47ec83602097b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfad0652a82f49a39a88b266ac7bc875","placeholder":"","style":"IPY_MODEL_59c9726e4898484b84b607a4d0345b5e","value":" 44868/44868 [02:29&lt;00:00, 291.22it/s]"}},"4c0bfc575df64e82904a5d653e04aac1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ec47a4894ba40b4a453cf9418155e35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fdcdf47224c4862a9432fdc71347931":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e87f91f49cb4146b2a4920dafcf13fd","IPY_MODEL_67e4ba7e3c1f44e2a818d85497731f8f","IPY_MODEL_37c398f54c0d473eaa47ec83602097b5"],"layout":"IPY_MODEL_d21def6b586040b89dd9e4ea2e0e60fa"}},"59c9726e4898484b84b607a4d0345b5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ac46d50275e453d8da68aa4265b1509":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c411634a2b840c79ff19e61a1b4e0c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a56b0667f5904801aff412435acc0189","placeholder":"","style":"IPY_MODEL_4c0bfc575df64e82904a5d653e04aac1","value":"100%"}},"6371df6edf6e4882b18b6d81b621716e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"677fa584c1554ebb8d8a8ae550244b8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67e4ba7e3c1f44e2a818d85497731f8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6371df6edf6e4882b18b6d81b621716e","max":44868,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f84bbd5ec5434cdea7d3283d66cc4b35","value":44868}},"a56b0667f5904801aff412435acc0189":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfd6490b61bd4a3fa9e7ab73466edcd7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c411634a2b840c79ff19e61a1b4e0c2","IPY_MODEL_0e6fc92c70dc4911bbd7599b098586f8","IPY_MODEL_18bde72a856549e8a9a80134148000f8"],"layout":"IPY_MODEL_f00aaa05d1b74d8298036b80cdb11757"}},"cfad0652a82f49a39a88b266ac7bc875":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d21def6b586040b89dd9e4ea2e0e60fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e94cf890b0a943d0859c9e474485e4ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea40c6598b36489c89b39dafe041512f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f00aaa05d1b74d8298036b80cdb11757":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f84bbd5ec5434cdea7d3283d66cc4b35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe78dc5f74d44735b28db3ba1cda7942":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}
